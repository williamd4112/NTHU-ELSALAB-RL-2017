{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3-1: REINFORCE\n",
    "    In this lab, you need to implement a REINFORCE algorithm with Tensorflow and solve OpenAI Gym CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole_env import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Define the data structure of experience\n",
    "Experience = namedtuple('Experience', 'state action reward next_state done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```discount``` function to compute discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards, gamma):\n",
    "    '''\n",
    "    param rewards: a rewards numpy array\n",
    "    param gamma: discount factor\n",
    "    '''\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    # TODOï¼š Calculate discounted rewards\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```do_step``` function to collect step results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_step(env, policy):\n",
    "    '''\n",
    "    Collect a step from env with policy\n",
    "    \n",
    "    param env: RL Environment\n",
    "    param policy: a function parameterized by environment state, return a action\n",
    "    return a list (state, action, reward, next_state, done) with length 1\n",
    "    '''        \n",
    "    # Empty list\n",
    "    rollout = []\n",
    "    state = env.current_state()\n",
    "    action = policy(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    rollout.append(Experience(state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "        \n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```ActorCriticAgent``` following ```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent(object):\n",
    "    def __init__(self, sess, n_states, n_actions, n_hiddens, lr_a, lr_c, gamma):\n",
    "        '''\n",
    "        param sess: tf session\n",
    "        param n_states: dim of states\n",
    "        param n_actions: dim of actions space\n",
    "        param n_hiddens: dim of hidden state\n",
    "        param lr_a: learning rate of actor\n",
    "        param lr_c: learning rate of critic\n",
    "        param gamma: discount factor\n",
    "        '''\n",
    "        self.sess = sess\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "       \n",
    "        self.state = tf.placeholder(shape=[None, n_states], dtype=tf.float32)\n",
    "        self.value = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        # For value loss\n",
    "        self.td_target = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # For policy loss\n",
    "        self.td_error_in = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    " \n",
    "        # TODO: Actor\n",
    "\n",
    "\n",
    "        # TODO: Critic\n",
    "\n",
    "        \n",
    "        # TODO: TD-error\n",
    "\n",
    "        # TODO: Value loss\n",
    "           \n",
    "        self.train_op_critic = tf.train.AdamOptimizer(learning_rate=self.lr_c).minimize(self.value_loss)\n",
    "\n",
    "        # TODO: Policy loss\n",
    "\n",
    "        self.train_op_actor = tf.train.AdamOptimizer(learning_rate=self.lr_a).minimize(self.policy_loss) \n",
    "        \n",
    "    def act(self, s):\n",
    "        '''\n",
    "        param s: a np.ndarray with shape [n_batches, n_states]\n",
    "        return a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Softmax stochastic policy\n",
    "    \n",
    "    def estimate(self, s):\n",
    "        '''\n",
    "        param s: a np.ndarray with shape [n_batches, n_states]\n",
    "        return a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Critic output\n",
    "    \n",
    "    def train(self, rollout):\n",
    "        '''\n",
    "        param rollout: a list of experience\n",
    "        '''\n",
    "        states = np.array([ np.asarray(e.state) for e in rollout ])\n",
    "        actions = np.reshape(np.array([ e.action for e in rollout ]), [len(states),])\n",
    "        rewards = np.reshape(np.array([ e.reward for e in rollout ]), [len(states),])\n",
    "        next_states = np.array([ np.asarray(e.next_state) for e in rollout ])\n",
    "\n",
    "        value_s_next = self.estimate(next_states)\n",
    "        value_s_next = np.reshape(value_s_next, [len(next_states),])\n",
    "\n",
    "        # TODO: TD Target\n",
    "\n",
    "        td_error, _ = self.sess.run([self.td_error_out, self.train_op_critic], feed_dict={self.state: states,\n",
    "                                                                                        self.td_target: td_target})\n",
    "        self.sess.run(self.train_op_actor, feed_dict={self.state: states, \n",
    "                                                    self.action: actions,\n",
    "                                                    self.td_error_in: td_error})\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-16 18:18:30,373] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "LR_A = 0.001\n",
    "LR_C = 0.01\n",
    "GAMMA = 0.99\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "env = CartpoleEnvironment()\n",
    "agent = ActorCriticAgent(sess=sess, \n",
    "                       n_states=env.observation_space.shape[0],\n",
    "                       n_actions=env.action_space.n,\n",
    "                       n_hiddens=20,\n",
    "                       lr_a=LR_A,\n",
    "                       lr_c=LR_C,\n",
    "                       gamma=GAMMA)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    return agent.act([s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_history_reward(history):\n",
    "    arr = np.asarray(history)\n",
    "    return arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward = -9.000000, Mean reward (over 1 episodes) = -9.000000\n",
      "Episode 1: Reward = -8.000000, Mean reward (over 2 episodes) = -8.500000\n",
      "Episode 2: Reward = -7.000000, Mean reward (over 3 episodes) = -8.000000\n",
      "Episode 3: Reward = 4.000000, Mean reward (over 4 episodes) = -5.000000\n",
      "Episode 4: Reward = 9.000000, Mean reward (over 5 episodes) = -2.200000\n",
      "Episode 5: Reward = 1.000000, Mean reward (over 6 episodes) = -1.666667\n",
      "Episode 6: Reward = 10.000000, Mean reward (over 7 episodes) = 0.000000\n",
      "Episode 7: Reward = 13.000000, Mean reward (over 8 episodes) = 1.625000\n",
      "Episode 8: Reward = -11.000000, Mean reward (over 9 episodes) = 0.222222\n",
      "Episode 9: Reward = -11.000000, Mean reward (over 10 episodes) = -0.900000\n",
      "Episode 10: Reward = 22.000000, Mean reward (over 11 episodes) = 1.181818\n",
      "Episode 11: Reward = -2.000000, Mean reward (over 12 episodes) = 0.916667\n",
      "Episode 12: Reward = 2.000000, Mean reward (over 13 episodes) = 1.000000\n",
      "Episode 13: Reward = 1.000000, Mean reward (over 14 episodes) = 1.000000\n",
      "Episode 14: Reward = 7.000000, Mean reward (over 15 episodes) = 1.400000\n",
      "Episode 15: Reward = 7.000000, Mean reward (over 16 episodes) = 1.750000\n",
      "Episode 16: Reward = 2.000000, Mean reward (over 17 episodes) = 1.764706\n",
      "Episode 17: Reward = 10.000000, Mean reward (over 18 episodes) = 2.222222\n",
      "Episode 18: Reward = -9.000000, Mean reward (over 19 episodes) = 1.631579\n",
      "Episode 19: Reward = 3.000000, Mean reward (over 20 episodes) = 1.700000\n",
      "Episode 20: Reward = 0.000000, Mean reward (over 21 episodes) = 1.619048\n",
      "Episode 21: Reward = -10.000000, Mean reward (over 22 episodes) = 1.090909\n",
      "Episode 22: Reward = 0.000000, Mean reward (over 23 episodes) = 1.043478\n",
      "Episode 23: Reward = -5.000000, Mean reward (over 24 episodes) = 0.791667\n",
      "Episode 24: Reward = -5.000000, Mean reward (over 25 episodes) = 0.560000\n",
      "Episode 25: Reward = -5.000000, Mean reward (over 26 episodes) = 0.346154\n",
      "Episode 26: Reward = -3.000000, Mean reward (over 27 episodes) = 0.222222\n",
      "Episode 27: Reward = -2.000000, Mean reward (over 28 episodes) = 0.142857\n",
      "Episode 28: Reward = 29.000000, Mean reward (over 29 episodes) = 1.137931\n",
      "Episode 29: Reward = -9.000000, Mean reward (over 30 episodes) = 0.800000\n",
      "Episode 30: Reward = 28.000000, Mean reward (over 31 episodes) = 1.677419\n",
      "Episode 31: Reward = -1.000000, Mean reward (over 32 episodes) = 1.593750\n",
      "Episode 32: Reward = 0.000000, Mean reward (over 33 episodes) = 1.545455\n",
      "Episode 33: Reward = -3.000000, Mean reward (over 34 episodes) = 1.411765\n",
      "Episode 34: Reward = -5.000000, Mean reward (over 35 episodes) = 1.228571\n",
      "Episode 35: Reward = 3.000000, Mean reward (over 36 episodes) = 1.277778\n",
      "Episode 36: Reward = -1.000000, Mean reward (over 37 episodes) = 1.216216\n",
      "Episode 37: Reward = 7.000000, Mean reward (over 38 episodes) = 1.368421\n",
      "Episode 38: Reward = -8.000000, Mean reward (over 39 episodes) = 1.128205\n",
      "Episode 39: Reward = 11.000000, Mean reward (over 40 episodes) = 1.375000\n",
      "Episode 40: Reward = -6.000000, Mean reward (over 41 episodes) = 1.195122\n",
      "Episode 41: Reward = -3.000000, Mean reward (over 42 episodes) = 1.095238\n",
      "Episode 42: Reward = -6.000000, Mean reward (over 43 episodes) = 0.930233\n",
      "Episode 43: Reward = 2.000000, Mean reward (over 44 episodes) = 0.954545\n",
      "Episode 44: Reward = 8.000000, Mean reward (over 45 episodes) = 1.111111\n",
      "Episode 45: Reward = 0.000000, Mean reward (over 46 episodes) = 1.086957\n",
      "Episode 46: Reward = 16.000000, Mean reward (over 47 episodes) = 1.404255\n",
      "Episode 47: Reward = -5.000000, Mean reward (over 48 episodes) = 1.270833\n",
      "Episode 48: Reward = -5.000000, Mean reward (over 49 episodes) = 1.142857\n",
      "Episode 49: Reward = 6.000000, Mean reward (over 50 episodes) = 1.240000\n",
      "Episode 50: Reward = -5.000000, Mean reward (over 51 episodes) = 1.117647\n",
      "Episode 51: Reward = 16.000000, Mean reward (over 52 episodes) = 1.403846\n",
      "Episode 52: Reward = -1.000000, Mean reward (over 53 episodes) = 1.358491\n",
      "Episode 53: Reward = 14.000000, Mean reward (over 54 episodes) = 1.592593\n",
      "Episode 54: Reward = 4.000000, Mean reward (over 55 episodes) = 1.636364\n",
      "Episode 55: Reward = -3.000000, Mean reward (over 56 episodes) = 1.553571\n",
      "Episode 56: Reward = -4.000000, Mean reward (over 57 episodes) = 1.456140\n",
      "Episode 57: Reward = -7.000000, Mean reward (over 58 episodes) = 1.310345\n",
      "Episode 58: Reward = 1.000000, Mean reward (over 59 episodes) = 1.305085\n",
      "Episode 59: Reward = -11.000000, Mean reward (over 60 episodes) = 1.100000\n",
      "Episode 60: Reward = -1.000000, Mean reward (over 61 episodes) = 1.065574\n",
      "Episode 61: Reward = 2.000000, Mean reward (over 62 episodes) = 1.080645\n",
      "Episode 62: Reward = -7.000000, Mean reward (over 63 episodes) = 0.952381\n",
      "Episode 63: Reward = -12.000000, Mean reward (over 64 episodes) = 0.750000\n",
      "Episode 64: Reward = -5.000000, Mean reward (over 65 episodes) = 0.661538\n",
      "Episode 65: Reward = -11.000000, Mean reward (over 66 episodes) = 0.484848\n",
      "Episode 66: Reward = 7.000000, Mean reward (over 67 episodes) = 0.582090\n",
      "Episode 67: Reward = -5.000000, Mean reward (over 68 episodes) = 0.500000\n",
      "Episode 68: Reward = 0.000000, Mean reward (over 69 episodes) = 0.492754\n",
      "Episode 69: Reward = -12.000000, Mean reward (over 70 episodes) = 0.314286\n",
      "Episode 70: Reward = -6.000000, Mean reward (over 71 episodes) = 0.225352\n",
      "Episode 71: Reward = -10.000000, Mean reward (over 72 episodes) = 0.083333\n",
      "Episode 72: Reward = -12.000000, Mean reward (over 73 episodes) = -0.082192\n",
      "Episode 73: Reward = -9.000000, Mean reward (over 74 episodes) = -0.202703\n",
      "Episode 74: Reward = -11.000000, Mean reward (over 75 episodes) = -0.346667\n",
      "Episode 75: Reward = -1.000000, Mean reward (over 76 episodes) = -0.355263\n",
      "Episode 76: Reward = -12.000000, Mean reward (over 77 episodes) = -0.506494\n",
      "Episode 77: Reward = -2.000000, Mean reward (over 78 episodes) = -0.525641\n",
      "Episode 78: Reward = -5.000000, Mean reward (over 79 episodes) = -0.582278\n",
      "Episode 79: Reward = 6.000000, Mean reward (over 80 episodes) = -0.500000\n",
      "Episode 80: Reward = -9.000000, Mean reward (over 81 episodes) = -0.604938\n",
      "Episode 81: Reward = -1.000000, Mean reward (over 82 episodes) = -0.609756\n",
      "Episode 82: Reward = -6.000000, Mean reward (over 83 episodes) = -0.674699\n",
      "Episode 83: Reward = -8.000000, Mean reward (over 84 episodes) = -0.761905\n",
      "Episode 84: Reward = -10.000000, Mean reward (over 85 episodes) = -0.870588\n",
      "Episode 85: Reward = 4.000000, Mean reward (over 86 episodes) = -0.813953\n",
      "Episode 86: Reward = 4.000000, Mean reward (over 87 episodes) = -0.758621\n",
      "Episode 87: Reward = -3.000000, Mean reward (over 88 episodes) = -0.784091\n",
      "Episode 88: Reward = -3.000000, Mean reward (over 89 episodes) = -0.808989\n",
      "Episode 89: Reward = 7.000000, Mean reward (over 90 episodes) = -0.722222\n",
      "Episode 90: Reward = 9.000000, Mean reward (over 91 episodes) = -0.615385\n",
      "Episode 91: Reward = -6.000000, Mean reward (over 92 episodes) = -0.673913\n",
      "Episode 92: Reward = 19.000000, Mean reward (over 93 episodes) = -0.462366\n",
      "Episode 93: Reward = -7.000000, Mean reward (over 94 episodes) = -0.531915\n",
      "Episode 94: Reward = 41.000000, Mean reward (over 95 episodes) = -0.094737\n",
      "Episode 95: Reward = 4.000000, Mean reward (over 96 episodes) = -0.052083\n",
      "Episode 96: Reward = 21.000000, Mean reward (over 97 episodes) = 0.164948\n",
      "Episode 97: Reward = 17.000000, Mean reward (over 98 episodes) = 0.336735\n",
      "Episode 98: Reward = 8.000000, Mean reward (over 99 episodes) = 0.414141\n",
      "Episode 99: Reward = 7.000000, Mean reward (over 100 episodes) = 0.480000\n",
      "Episode 100: Reward = -8.000000, Mean reward (over 100 episodes) = 0.490000\n",
      "Episode 101: Reward = 1.000000, Mean reward (over 100 episodes) = 0.580000\n",
      "Episode 102: Reward = 13.000000, Mean reward (over 100 episodes) = 0.780000\n",
      "Episode 103: Reward = 1.000000, Mean reward (over 100 episodes) = 0.750000\n",
      "Episode 104: Reward = 18.000000, Mean reward (over 100 episodes) = 0.840000\n",
      "Episode 105: Reward = 17.000000, Mean reward (over 100 episodes) = 1.000000\n",
      "Episode 106: Reward = 16.000000, Mean reward (over 100 episodes) = 1.060000\n",
      "Episode 107: Reward = -1.000000, Mean reward (over 100 episodes) = 0.920000\n",
      "Episode 108: Reward = 8.000000, Mean reward (over 100 episodes) = 1.110000\n",
      "Episode 109: Reward = 33.000000, Mean reward (over 100 episodes) = 1.550000\n",
      "Episode 110: Reward = 55.000000, Mean reward (over 100 episodes) = 1.880000\n",
      "Episode 111: Reward = 22.000000, Mean reward (over 100 episodes) = 2.120000\n",
      "Episode 112: Reward = 0.000000, Mean reward (over 100 episodes) = 2.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 113: Reward = 13.000000, Mean reward (over 100 episodes) = 2.220000\n",
      "Episode 114: Reward = -8.000000, Mean reward (over 100 episodes) = 2.070000\n",
      "Episode 115: Reward = -7.000000, Mean reward (over 100 episodes) = 1.930000\n",
      "Episode 116: Reward = 11.000000, Mean reward (over 100 episodes) = 2.020000\n",
      "Episode 117: Reward = 11.000000, Mean reward (over 100 episodes) = 2.030000\n",
      "Episode 118: Reward = 20.000000, Mean reward (over 100 episodes) = 2.320000\n",
      "Episode 119: Reward = 33.000000, Mean reward (over 100 episodes) = 2.620000\n",
      "Episode 120: Reward = 29.000000, Mean reward (over 100 episodes) = 2.910000\n",
      "Episode 121: Reward = 9.000000, Mean reward (over 100 episodes) = 3.100000\n",
      "Episode 122: Reward = 32.000000, Mean reward (over 100 episodes) = 3.420000\n",
      "Episode 123: Reward = 41.000000, Mean reward (over 100 episodes) = 3.880000\n",
      "Episode 124: Reward = 25.000000, Mean reward (over 100 episodes) = 4.180000\n",
      "Episode 125: Reward = 33.000000, Mean reward (over 100 episodes) = 4.560000\n",
      "Episode 126: Reward = 28.000000, Mean reward (over 100 episodes) = 4.870000\n",
      "Episode 127: Reward = 25.000000, Mean reward (over 100 episodes) = 5.140000\n",
      "Episode 128: Reward = 22.000000, Mean reward (over 100 episodes) = 5.070000\n",
      "Episode 129: Reward = 7.000000, Mean reward (over 100 episodes) = 5.230000\n",
      "Episode 130: Reward = 11.000000, Mean reward (over 100 episodes) = 5.060000\n",
      "Episode 131: Reward = 10.000000, Mean reward (over 100 episodes) = 5.170000\n",
      "Episode 132: Reward = 7.000000, Mean reward (over 100 episodes) = 5.240000\n",
      "Episode 133: Reward = 6.000000, Mean reward (over 100 episodes) = 5.330000\n",
      "Episode 134: Reward = 33.000000, Mean reward (over 100 episodes) = 5.710000\n",
      "Episode 135: Reward = 30.000000, Mean reward (over 100 episodes) = 5.980000\n",
      "Episode 136: Reward = 30.000000, Mean reward (over 100 episodes) = 6.290000\n",
      "Episode 137: Reward = 11.000000, Mean reward (over 100 episodes) = 6.330000\n",
      "Episode 138: Reward = 17.000000, Mean reward (over 100 episodes) = 6.580000\n",
      "Episode 139: Reward = 30.000000, Mean reward (over 100 episodes) = 6.770000\n",
      "Episode 140: Reward = 45.000000, Mean reward (over 100 episodes) = 7.280000\n",
      "Episode 141: Reward = 63.000000, Mean reward (over 100 episodes) = 7.940000\n",
      "Episode 142: Reward = 11.000000, Mean reward (over 100 episodes) = 8.110000\n",
      "Episode 143: Reward = 40.000000, Mean reward (over 100 episodes) = 8.490000\n",
      "Episode 144: Reward = 26.000000, Mean reward (over 100 episodes) = 8.670000\n",
      "Episode 145: Reward = 55.000000, Mean reward (over 100 episodes) = 9.220000\n",
      "Episode 146: Reward = 52.000000, Mean reward (over 100 episodes) = 9.580000\n",
      "Episode 147: Reward = 43.000000, Mean reward (over 100 episodes) = 10.060000\n",
      "Episode 148: Reward = 51.000000, Mean reward (over 100 episodes) = 10.620000\n",
      "Episode 149: Reward = 27.000000, Mean reward (over 100 episodes) = 10.830000\n",
      "Episode 150: Reward = 50.000000, Mean reward (over 100 episodes) = 11.380000\n",
      "Episode 151: Reward = 20.000000, Mean reward (over 100 episodes) = 11.420000\n",
      "Episode 152: Reward = 79.000000, Mean reward (over 100 episodes) = 12.220000\n",
      "Episode 153: Reward = 35.000000, Mean reward (over 100 episodes) = 12.430000\n",
      "Episode 154: Reward = 41.000000, Mean reward (over 100 episodes) = 12.800000\n",
      "Episode 155: Reward = 107.000000, Mean reward (over 100 episodes) = 13.900000\n",
      "Episode 156: Reward = 116.000000, Mean reward (over 100 episodes) = 15.100000\n",
      "Episode 157: Reward = 18.000000, Mean reward (over 100 episodes) = 15.350000\n",
      "Episode 158: Reward = 47.000000, Mean reward (over 100 episodes) = 15.810000\n",
      "Episode 159: Reward = 46.000000, Mean reward (over 100 episodes) = 16.380000\n",
      "Episode 160: Reward = 91.000000, Mean reward (over 100 episodes) = 17.300000\n",
      "Episode 161: Reward = 28.000000, Mean reward (over 100 episodes) = 17.560000\n",
      "Episode 162: Reward = 62.000000, Mean reward (over 100 episodes) = 18.250000\n",
      "Episode 163: Reward = 8.000000, Mean reward (over 100 episodes) = 18.450000\n",
      "Episode 164: Reward = 37.000000, Mean reward (over 100 episodes) = 18.870000\n",
      "Episode 165: Reward = 13.000000, Mean reward (over 100 episodes) = 19.110000\n",
      "Episode 166: Reward = 39.000000, Mean reward (over 100 episodes) = 19.430000\n",
      "Episode 167: Reward = 17.000000, Mean reward (over 100 episodes) = 19.650000\n",
      "Episode 168: Reward = 6.000000, Mean reward (over 100 episodes) = 19.710000\n",
      "Episode 169: Reward = -2.000000, Mean reward (over 100 episodes) = 19.810000\n",
      "Episode 170: Reward = 0.000000, Mean reward (over 100 episodes) = 19.870000\n",
      "Episode 171: Reward = -1.000000, Mean reward (over 100 episodes) = 19.960000\n",
      "Episode 172: Reward = -5.000000, Mean reward (over 100 episodes) = 20.030000\n",
      "Episode 173: Reward = -4.000000, Mean reward (over 100 episodes) = 20.080000\n",
      "Episode 174: Reward = -4.000000, Mean reward (over 100 episodes) = 20.150000\n",
      "Episode 175: Reward = -3.000000, Mean reward (over 100 episodes) = 20.130000\n",
      "Episode 176: Reward = 3.000000, Mean reward (over 100 episodes) = 20.280000\n",
      "Episode 177: Reward = 8.000000, Mean reward (over 100 episodes) = 20.380000\n",
      "Episode 178: Reward = 2.000000, Mean reward (over 100 episodes) = 20.450000\n",
      "Episode 179: Reward = 10.000000, Mean reward (over 100 episodes) = 20.490000\n",
      "Episode 180: Reward = 3.000000, Mean reward (over 100 episodes) = 20.610000\n",
      "Episode 181: Reward = 3.000000, Mean reward (over 100 episodes) = 20.650000\n",
      "Episode 182: Reward = 7.000000, Mean reward (over 100 episodes) = 20.780000\n",
      "Episode 183: Reward = 6.000000, Mean reward (over 100 episodes) = 20.920000\n",
      "Episode 184: Reward = -1.000000, Mean reward (over 100 episodes) = 21.010000\n",
      "Episode 185: Reward = 18.000000, Mean reward (over 100 episodes) = 21.150000\n",
      "Episode 186: Reward = 8.000000, Mean reward (over 100 episodes) = 21.190000\n",
      "Episode 187: Reward = 22.000000, Mean reward (over 100 episodes) = 21.440000\n",
      "Episode 188: Reward = 11.000000, Mean reward (over 100 episodes) = 21.580000\n",
      "Episode 189: Reward = 26.000000, Mean reward (over 100 episodes) = 21.770000\n",
      "Episode 190: Reward = 43.000000, Mean reward (over 100 episodes) = 22.110000\n",
      "Episode 191: Reward = 30.000000, Mean reward (over 100 episodes) = 22.470000\n",
      "Episode 192: Reward = 29.000000, Mean reward (over 100 episodes) = 22.570000\n",
      "Episode 193: Reward = 29.000000, Mean reward (over 100 episodes) = 22.930000\n",
      "Episode 194: Reward = 37.000000, Mean reward (over 100 episodes) = 22.890000\n",
      "Episode 195: Reward = 3.000000, Mean reward (over 100 episodes) = 22.880000\n",
      "Episode 196: Reward = 40.000000, Mean reward (over 100 episodes) = 23.070000\n",
      "Episode 197: Reward = 11.000000, Mean reward (over 100 episodes) = 23.010000\n",
      "Episode 198: Reward = 14.000000, Mean reward (over 100 episodes) = 23.070000\n",
      "Episode 199: Reward = 7.000000, Mean reward (over 100 episodes) = 23.070000\n",
      "Episode 200: Reward = 8.000000, Mean reward (over 100 episodes) = 23.230000\n",
      "Episode 201: Reward = 20.000000, Mean reward (over 100 episodes) = 23.420000\n",
      "Episode 202: Reward = 3.000000, Mean reward (over 100 episodes) = 23.320000\n",
      "Episode 203: Reward = 7.000000, Mean reward (over 100 episodes) = 23.380000\n",
      "Episode 204: Reward = -2.000000, Mean reward (over 100 episodes) = 23.180000\n",
      "Episode 205: Reward = 10.000000, Mean reward (over 100 episodes) = 23.110000\n",
      "Episode 206: Reward = 5.000000, Mean reward (over 100 episodes) = 23.000000\n",
      "Episode 207: Reward = -4.000000, Mean reward (over 100 episodes) = 22.970000\n",
      "Episode 208: Reward = -8.000000, Mean reward (over 100 episodes) = 22.810000\n",
      "Episode 209: Reward = -6.000000, Mean reward (over 100 episodes) = 22.420000\n",
      "Episode 210: Reward = -3.000000, Mean reward (over 100 episodes) = 21.840000\n",
      "Episode 211: Reward = -3.000000, Mean reward (over 100 episodes) = 21.590000\n",
      "Episode 212: Reward = 6.000000, Mean reward (over 100 episodes) = 21.650000\n",
      "Episode 213: Reward = -7.000000, Mean reward (over 100 episodes) = 21.450000\n",
      "Episode 214: Reward = -3.000000, Mean reward (over 100 episodes) = 21.500000\n",
      "Episode 215: Reward = -11.000000, Mean reward (over 100 episodes) = 21.460000\n",
      "Episode 216: Reward = 0.000000, Mean reward (over 100 episodes) = 21.350000\n",
      "Episode 217: Reward = -3.000000, Mean reward (over 100 episodes) = 21.210000\n",
      "Episode 218: Reward = -1.000000, Mean reward (over 100 episodes) = 21.000000\n",
      "Episode 219: Reward = 7.000000, Mean reward (over 100 episodes) = 20.740000\n",
      "Episode 220: Reward = 0.000000, Mean reward (over 100 episodes) = 20.450000\n",
      "Episode 221: Reward = 9.000000, Mean reward (over 100 episodes) = 20.450000\n",
      "Episode 222: Reward = 2.000000, Mean reward (over 100 episodes) = 20.150000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 223: Reward = -2.000000, Mean reward (over 100 episodes) = 19.720000\n",
      "Episode 224: Reward = 11.000000, Mean reward (over 100 episodes) = 19.580000\n",
      "Episode 225: Reward = 10.000000, Mean reward (over 100 episodes) = 19.350000\n",
      "Episode 226: Reward = 8.000000, Mean reward (over 100 episodes) = 19.150000\n",
      "Episode 227: Reward = -1.000000, Mean reward (over 100 episodes) = 18.890000\n",
      "Episode 228: Reward = 5.000000, Mean reward (over 100 episodes) = 18.720000\n",
      "Episode 229: Reward = 1.000000, Mean reward (over 100 episodes) = 18.660000\n",
      "Episode 230: Reward = -4.000000, Mean reward (over 100 episodes) = 18.510000\n",
      "Episode 231: Reward = -2.000000, Mean reward (over 100 episodes) = 18.390000\n",
      "Episode 232: Reward = 3.000000, Mean reward (over 100 episodes) = 18.350000\n",
      "Episode 233: Reward = -11.000000, Mean reward (over 100 episodes) = 18.180000\n",
      "Episode 234: Reward = -5.000000, Mean reward (over 100 episodes) = 17.800000\n",
      "Episode 235: Reward = -2.000000, Mean reward (over 100 episodes) = 17.480000\n",
      "Episode 236: Reward = 10.000000, Mean reward (over 100 episodes) = 17.280000\n",
      "Episode 237: Reward = 1.000000, Mean reward (over 100 episodes) = 17.180000\n",
      "Episode 238: Reward = -5.000000, Mean reward (over 100 episodes) = 16.960000\n",
      "Episode 239: Reward = -8.000000, Mean reward (over 100 episodes) = 16.580000\n",
      "Episode 240: Reward = 4.000000, Mean reward (over 100 episodes) = 16.170000\n",
      "Episode 241: Reward = 17.000000, Mean reward (over 100 episodes) = 15.710000\n",
      "Episode 242: Reward = 6.000000, Mean reward (over 100 episodes) = 15.660000\n",
      "Episode 243: Reward = 22.000000, Mean reward (over 100 episodes) = 15.480000\n",
      "Episode 244: Reward = -1.000000, Mean reward (over 100 episodes) = 15.210000\n",
      "Episode 245: Reward = 22.000000, Mean reward (over 100 episodes) = 14.880000\n",
      "Episode 246: Reward = 11.000000, Mean reward (over 100 episodes) = 14.470000\n",
      "Episode 247: Reward = 13.000000, Mean reward (over 100 episodes) = 14.170000\n",
      "Episode 248: Reward = 21.000000, Mean reward (over 100 episodes) = 13.870000\n",
      "Episode 249: Reward = 17.000000, Mean reward (over 100 episodes) = 13.770000\n",
      "Episode 250: Reward = 19.000000, Mean reward (over 100 episodes) = 13.460000\n",
      "Episode 251: Reward = 27.000000, Mean reward (over 100 episodes) = 13.530000\n",
      "Episode 252: Reward = 23.000000, Mean reward (over 100 episodes) = 12.970000\n",
      "Episode 253: Reward = 40.000000, Mean reward (over 100 episodes) = 13.020000\n",
      "Episode 254: Reward = 30.000000, Mean reward (over 100 episodes) = 12.910000\n",
      "Episode 255: Reward = 99.000000, Mean reward (over 100 episodes) = 12.830000\n",
      "Episode 256: Reward = 33.000000, Mean reward (over 100 episodes) = 12.000000\n",
      "Episode 257: Reward = 10.000000, Mean reward (over 100 episodes) = 11.920000\n",
      "Episode 258: Reward = 9.000000, Mean reward (over 100 episodes) = 11.540000\n",
      "Episode 259: Reward = 5.000000, Mean reward (over 100 episodes) = 11.130000\n",
      "Episode 260: Reward = -4.000000, Mean reward (over 100 episodes) = 10.180000\n",
      "Episode 261: Reward = 9.000000, Mean reward (over 100 episodes) = 9.990000\n",
      "Episode 262: Reward = 2.000000, Mean reward (over 100 episodes) = 9.390000\n",
      "Episode 263: Reward = 2.000000, Mean reward (over 100 episodes) = 9.330000\n",
      "Episode 264: Reward = 2.000000, Mean reward (over 100 episodes) = 8.980000\n",
      "Episode 265: Reward = -5.000000, Mean reward (over 100 episodes) = 8.800000\n",
      "Episode 266: Reward = 21.000000, Mean reward (over 100 episodes) = 8.620000\n",
      "Episode 267: Reward = 9.000000, Mean reward (over 100 episodes) = 8.540000\n",
      "Episode 268: Reward = 7.000000, Mean reward (over 100 episodes) = 8.550000\n",
      "Episode 269: Reward = 4.000000, Mean reward (over 100 episodes) = 8.610000\n",
      "Episode 270: Reward = 24.000000, Mean reward (over 100 episodes) = 8.850000\n",
      "Episode 271: Reward = 13.000000, Mean reward (over 100 episodes) = 8.990000\n",
      "Episode 272: Reward = 22.000000, Mean reward (over 100 episodes) = 9.260000\n",
      "Episode 273: Reward = -2.000000, Mean reward (over 100 episodes) = 9.280000\n",
      "Episode 274: Reward = 19.000000, Mean reward (over 100 episodes) = 9.510000\n",
      "Episode 275: Reward = 11.000000, Mean reward (over 100 episodes) = 9.650000\n",
      "Episode 276: Reward = 8.000000, Mean reward (over 100 episodes) = 9.700000\n",
      "Episode 277: Reward = 1.000000, Mean reward (over 100 episodes) = 9.630000\n",
      "Episode 278: Reward = 4.000000, Mean reward (over 100 episodes) = 9.650000\n",
      "Episode 279: Reward = 2.000000, Mean reward (over 100 episodes) = 9.570000\n",
      "Episode 280: Reward = 34.000000, Mean reward (over 100 episodes) = 9.880000\n",
      "Episode 281: Reward = 76.000000, Mean reward (over 100 episodes) = 10.610000\n",
      "Episode 282: Reward = 83.000000, Mean reward (over 100 episodes) = 11.370000\n",
      "Episode 283: Reward = 118.000000, Mean reward (over 100 episodes) = 12.490000\n",
      "Episode 284: Reward = 67.000000, Mean reward (over 100 episodes) = 13.170000\n",
      "Episode 285: Reward = 25.000000, Mean reward (over 100 episodes) = 13.240000\n",
      "Episode 286: Reward = 24.000000, Mean reward (over 100 episodes) = 13.400000\n",
      "Episode 287: Reward = 24.000000, Mean reward (over 100 episodes) = 13.420000\n",
      "Episode 288: Reward = 9.000000, Mean reward (over 100 episodes) = 13.400000\n",
      "Episode 289: Reward = 9.000000, Mean reward (over 100 episodes) = 13.230000\n",
      "Episode 290: Reward = -2.000000, Mean reward (over 100 episodes) = 12.780000\n",
      "Episode 291: Reward = -2.000000, Mean reward (over 100 episodes) = 12.460000\n",
      "Episode 292: Reward = 5.000000, Mean reward (over 100 episodes) = 12.220000\n",
      "Episode 293: Reward = -8.000000, Mean reward (over 100 episodes) = 11.850000\n",
      "Episode 294: Reward = -1.000000, Mean reward (over 100 episodes) = 11.470000\n",
      "Episode 295: Reward = 0.000000, Mean reward (over 100 episodes) = 11.440000\n",
      "Episode 296: Reward = -4.000000, Mean reward (over 100 episodes) = 11.000000\n",
      "Episode 297: Reward = 6.000000, Mean reward (over 100 episodes) = 10.950000\n",
      "Episode 298: Reward = 22.000000, Mean reward (over 100 episodes) = 11.030000\n",
      "Episode 299: Reward = 110.000000, Mean reward (over 100 episodes) = 12.060000\n",
      "Episode 300: Reward = 200.000000, Mean reward (over 100 episodes) = 13.980000\n",
      "Episode 301: Reward = 200.000000, Mean reward (over 100 episodes) = 15.780000\n",
      "Episode 302: Reward = 200.000000, Mean reward (over 100 episodes) = 17.750000\n",
      "Episode 303: Reward = 200.000000, Mean reward (over 100 episodes) = 19.680000\n",
      "Episode 304: Reward = 200.000000, Mean reward (over 100 episodes) = 21.700000\n",
      "Episode 305: Reward = 19.000000, Mean reward (over 100 episodes) = 21.790000\n",
      "Episode 306: Reward = 161.000000, Mean reward (over 100 episodes) = 23.350000\n",
      "Episode 307: Reward = 200.000000, Mean reward (over 100 episodes) = 25.390000\n",
      "Episode 308: Reward = 200.000000, Mean reward (over 100 episodes) = 27.470000\n",
      "Episode 309: Reward = 200.000000, Mean reward (over 100 episodes) = 29.530000\n",
      "Episode 310: Reward = 200.000000, Mean reward (over 100 episodes) = 31.560000\n",
      "Episode 311: Reward = 200.000000, Mean reward (over 100 episodes) = 33.590000\n",
      "Episode 312: Reward = 200.000000, Mean reward (over 100 episodes) = 35.530000\n",
      "Episode 313: Reward = 200.000000, Mean reward (over 100 episodes) = 37.600000\n",
      "Episode 314: Reward = 200.000000, Mean reward (over 100 episodes) = 39.630000\n",
      "Episode 315: Reward = 200.000000, Mean reward (over 100 episodes) = 41.740000\n",
      "Episode 316: Reward = 200.000000, Mean reward (over 100 episodes) = 43.740000\n",
      "Episode 317: Reward = 200.000000, Mean reward (over 100 episodes) = 45.770000\n",
      "Episode 318: Reward = 140.000000, Mean reward (over 100 episodes) = 47.180000\n",
      "Episode 319: Reward = 60.000000, Mean reward (over 100 episodes) = 47.710000\n",
      "Episode 320: Reward = 200.000000, Mean reward (over 100 episodes) = 49.710000\n",
      "Episode 321: Reward = 200.000000, Mean reward (over 100 episodes) = 51.620000\n",
      "Episode 322: Reward = 200.000000, Mean reward (over 100 episodes) = 53.600000\n",
      "Episode 323: Reward = 200.000000, Mean reward (over 100 episodes) = 55.620000\n",
      "Episode 324: Reward = 200.000000, Mean reward (over 100 episodes) = 57.510000\n",
      "Episode 325: Reward = 200.000000, Mean reward (over 100 episodes) = 59.410000\n",
      "Episode 326: Reward = 200.000000, Mean reward (over 100 episodes) = 61.330000\n",
      "Episode 327: Reward = 200.000000, Mean reward (over 100 episodes) = 63.340000\n",
      "Episode 328: Reward = 200.000000, Mean reward (over 100 episodes) = 65.290000\n",
      "Episode 329: Reward = 200.000000, Mean reward (over 100 episodes) = 67.280000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 330: Reward = 200.000000, Mean reward (over 100 episodes) = 69.320000\n",
      "Episode 331: Reward = 200.000000, Mean reward (over 100 episodes) = 71.340000\n",
      "Episode 332: Reward = 200.000000, Mean reward (over 100 episodes) = 73.310000\n",
      "Episode 333: Reward = 200.000000, Mean reward (over 100 episodes) = 75.420000\n",
      "Episode 334: Reward = 200.000000, Mean reward (over 100 episodes) = 77.470000\n",
      "Episode 335: Reward = 200.000000, Mean reward (over 100 episodes) = 79.490000\n",
      "Episode 336: Reward = 200.000000, Mean reward (over 100 episodes) = 81.390000\n",
      "Episode 337: Reward = 200.000000, Mean reward (over 100 episodes) = 83.380000\n",
      "Episode 338: Reward = 200.000000, Mean reward (over 100 episodes) = 85.430000\n",
      "Episode 339: Reward = 200.000000, Mean reward (over 100 episodes) = 87.510000\n",
      "Episode 340: Reward = 200.000000, Mean reward (over 100 episodes) = 89.470000\n",
      "Episode 341: Reward = 200.000000, Mean reward (over 100 episodes) = 91.300000\n",
      "Episode 342: Reward = 200.000000, Mean reward (over 100 episodes) = 93.240000\n",
      "Episode 343: Reward = 200.000000, Mean reward (over 100 episodes) = 95.020000\n",
      "Episode 344: Reward = 200.000000, Mean reward (over 100 episodes) = 97.030000\n",
      "Episode 345: Reward = 200.000000, Mean reward (over 100 episodes) = 98.810000\n",
      "Episode 346: Reward = 200.000000, Mean reward (over 100 episodes) = 100.700000\n",
      "Episode 347: Reward = 200.000000, Mean reward (over 100 episodes) = 102.570000\n",
      "Episode 348: Reward = 200.000000, Mean reward (over 100 episodes) = 104.360000\n",
      "Episode 349: Reward = 200.000000, Mean reward (over 100 episodes) = 106.190000\n",
      "Episode 350: Reward = 200.000000, Mean reward (over 100 episodes) = 108.000000\n",
      "Episode 351: Reward = 200.000000, Mean reward (over 100 episodes) = 109.730000\n",
      "Episode 352: Reward = 200.000000, Mean reward (over 100 episodes) = 111.500000\n",
      "Episode 353: Reward = 200.000000, Mean reward (over 100 episodes) = 113.100000\n",
      "Episode 354: Reward = 200.000000, Mean reward (over 100 episodes) = 114.800000\n",
      "Episode 355: Reward = 200.000000, Mean reward (over 100 episodes) = 115.810000\n",
      "Episode 356: Reward = 200.000000, Mean reward (over 100 episodes) = 117.480000\n",
      "Episode 357: Reward = 200.000000, Mean reward (over 100 episodes) = 119.380000\n",
      "Episode 358: Reward = 200.000000, Mean reward (over 100 episodes) = 121.290000\n",
      "Episode 359: Reward = 200.000000, Mean reward (over 100 episodes) = 123.240000\n",
      "Episode 360: Reward = 200.000000, Mean reward (over 100 episodes) = 125.280000\n",
      "Episode 361: Reward = 200.000000, Mean reward (over 100 episodes) = 127.190000\n",
      "Episode 362: Reward = 200.000000, Mean reward (over 100 episodes) = 129.170000\n",
      "Episode 363: Reward = 200.000000, Mean reward (over 100 episodes) = 131.150000\n",
      "Episode 364: Reward = 200.000000, Mean reward (over 100 episodes) = 133.130000\n",
      "Episode 365: Reward = 200.000000, Mean reward (over 100 episodes) = 135.180000\n",
      "Episode 366: Reward = 200.000000, Mean reward (over 100 episodes) = 136.970000\n",
      "Episode 367: Reward = 200.000000, Mean reward (over 100 episodes) = 138.880000\n",
      "Episode 368: Reward = 200.000000, Mean reward (over 100 episodes) = 140.810000\n",
      "Episode 369: Reward = 200.000000, Mean reward (over 100 episodes) = 142.770000\n",
      "Episode 370: Reward = 200.000000, Mean reward (over 100 episodes) = 144.530000\n",
      "Episode 371: Reward = 200.000000, Mean reward (over 100 episodes) = 146.400000\n",
      "Episode 372: Reward = 200.000000, Mean reward (over 100 episodes) = 148.180000\n",
      "Episode 373: Reward = 200.000000, Mean reward (over 100 episodes) = 150.200000\n",
      "Episode 374: Reward = 200.000000, Mean reward (over 100 episodes) = 152.010000\n",
      "Episode 375: Reward = 200.000000, Mean reward (over 100 episodes) = 153.900000\n",
      "Episode 376: Reward = 200.000000, Mean reward (over 100 episodes) = 155.820000\n",
      "Episode 377: Reward = 200.000000, Mean reward (over 100 episodes) = 157.810000\n",
      "Episode 378: Reward = 200.000000, Mean reward (over 100 episodes) = 159.770000\n",
      "Episode 379: Reward = 200.000000, Mean reward (over 100 episodes) = 161.750000\n",
      "Episode 380: Reward = 200.000000, Mean reward (over 100 episodes) = 163.410000\n",
      "Episode 381: Reward = 200.000000, Mean reward (over 100 episodes) = 164.650000\n",
      "Episode 382: Reward = 200.000000, Mean reward (over 100 episodes) = 165.820000\n",
      "Episode 383: Reward = 200.000000, Mean reward (over 100 episodes) = 166.640000\n",
      "Episode 384: Reward = 200.000000, Mean reward (over 100 episodes) = 167.970000\n",
      "Episode 385: Reward = 200.000000, Mean reward (over 100 episodes) = 169.720000\n",
      "Episode 386: Reward = 200.000000, Mean reward (over 100 episodes) = 171.480000\n",
      "Episode 387: Reward = 200.000000, Mean reward (over 100 episodes) = 173.240000\n",
      "Episode 388: Reward = 200.000000, Mean reward (over 100 episodes) = 175.150000\n",
      "Episode 389: Reward = 200.000000, Mean reward (over 100 episodes) = 177.060000\n",
      "Episode 390: Reward = 200.000000, Mean reward (over 100 episodes) = 179.080000\n",
      "Episode 391: Reward = 200.000000, Mean reward (over 100 episodes) = 181.100000\n",
      "Episode 392: Reward = 200.000000, Mean reward (over 100 episodes) = 183.050000\n",
      "Episode 393: Reward = 200.000000, Mean reward (over 100 episodes) = 185.130000\n",
      "Episode 394: Reward = 200.000000, Mean reward (over 100 episodes) = 187.140000\n",
      "Episode 395: Reward = 200.000000, Mean reward (over 100 episodes) = 189.140000\n",
      "Episode 396: Reward = 200.000000, Mean reward (over 100 episodes) = 191.180000\n",
      "Episode 397: Reward = 200.000000, Mean reward (over 100 episodes) = 193.120000\n",
      "Episode 398: Reward = 200.000000, Mean reward (over 100 episodes) = 194.900000\n",
      "Episode 399: Reward = 200.000000, Mean reward (over 100 episodes) = 195.800000\n",
      "Pass\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 500000\n",
    "\n",
    "episode_reward = 0.0\n",
    "history_episode_rewards = deque(maxlen=100)\n",
    "episode = 0\n",
    "\n",
    "plot_history_episode_rewards = []\n",
    "\n",
    "env.reset()\n",
    "for iter in range(MAX_ITERATIONS):\n",
    "    rollout = do_step(env=env, policy=policy)\n",
    "    agent.train(rollout=rollout)\n",
    "    \n",
    "    episode_reward += rollout[0].reward\n",
    "    if rollout[0].done:\n",
    "        history_episode_rewards.append(episode_reward)\n",
    "        plot_history_episode_rewards.append(episode_reward)\n",
    "        mean_rewards = eval_history_reward(history_episode_rewards)\n",
    "        print('Episode %d: Reward = %f, Mean reward (over %d episodes) = %f' % (episode, \n",
    "                                                                                 episode_reward,\n",
    "                                                                                 len(history_episode_rewards),\n",
    "                                                                                 mean_rewards))\n",
    "        env.reset()\n",
    "        episode += 1\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        if mean_rewards > 195.0:\n",
    "            print('Pass')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmcHGW193+nl5mefUkmk8memBCSQIAkhB2HVRZfkUUF\nfZXrxlXAq696L+CKXhFcuF69elFABEE2BUQJIhAYlgQSskwWErJPlskkk0wyS8/SWz3vH1VPdVV1\nVXd1T9d0hznfz2c+011L15mqnuc8Z31ICAGGYRiGseIrtAAMwzBMccIKgmEYhrGFFQTDMAxjCysI\nhmEYxhZWEAzDMIwtrCAYhmEYW1hBMAzDMLawgmAYhmFsYQXBMAzD2BIotADDYezYsWLatGk5ndvf\n34+Kior8CpQnilU2lis7WK7sYLmyJ1fZVq9efVgI0ZDxQCHEMfuzcOFCkSuvvvpqzud6TbHKxnJl\nB8uVHSxX9uQqG4BVwsUYyy4mhmEYxhZWEAzDMIwtrCAYhmEYW1hBMAzDMLawgmAYhmFs8UxBENFk\nInqViDYR0btE9FVtez0RvURE27Tfddp2IqJfEdF2IlpPRAu8ko1hGIbJjJcWRBzAN4QQcwGcDuAm\nIpoL4FYAS4UQswAs1d4DwKUAZmk/NwC4x0PZGIZhmAx4VignhOgA0KG97iOizQAmArgCQLN22EMA\nWgDcom3/o5aj+zYR1RJRk/Y5DMO8j9nfPYi/bo9iTXQLAGBCbRk+cepkPLi8DUf7o47nfWBcJS49\noQkPLt+F8FDcE9nadiflKiYWTav3/BokRmBNaiKaBuB1ACcA2COEqNW2E4CjQohaInoOwF1CiDe1\nfUsB3CKEWGX5rBugWhhobGxc+Pjjj+ckUzgcRmVlZW5/kMcUq2wsV3awXO75244ont4WM227/YwQ\nbn9rCABANucIAAECbl0cwo9WOB83fIRnnzwcLpsexKUTozk9y/POO2+1EGJRxgPdVNMN5wdAJYDV\nAK7S3ndb9h/Vfj8H4GzD9qUAFqX7bK6kHllYruxgudxz94tbxNRbnhOKoojfv7FTTL3lOfHWjsNi\n6i3PiefX7097ztvacW9uO+SJbMV4vyTHdCU1EQUBPAXgT0KIp7XNB4moSdvfBKBT294OYLLh9Ena\nNoZh3ucIIUAAiAikTdZjCQUAEPDbD1N+7cCEonpBqPgm+cc8XmYxEYDfA9gshPgvw66/Abhee309\ngGcN2z+jZTOdDqBHcPyBYUYFihD6AC/H+WhcVRBBv/3IH9C2x6SCKEI30LGOl91czwLwaQAbiKhV\n2/YtAHcBeJKIPg9gN4CPa/ueB3AZgO0ABgB81kPZGIYpIhJKcrZKmqaQFkTQwYLwacfFtePYgsg/\nXmYxvQnnyM4FNscLADd5JQ/DMMWLMFoQ2u9oQrUMAj4HC8InFYl6nI81RN7hSmqGYQpOWhdTwMGC\n0BREXGELwitYQTAMU3DsXEy6gvDZD1PSgojrFoS3Mo5GWEEwDFNwFDsXUzwBIBmMtuLzmWMVxVir\ncKzDCoJhmIIjhNAtAJmNJGMLTkFq3YJQ2ILwClYQDMMUnIShKCsZpE6f5upPyWJiDZFvWEEwDFNw\nFIE0dRAOhXIpWUyeijgqYQXBMEzBUdNc1RE+tZLawYKwZjFxDCLvsIJgGKbgJBThXCjnkMXkt8Qg\n2MOUf1hBMAxTcNK6mBzqIPyWNFdWEPmHFQTDMAVHUYSuGPQ6COlicgguJBWEehxXUucfVhAMwxQc\nxZTmqhKNp09zlVlMMXYxeQYrCIZhCo4ikllIxjRXHyUtBSt+P1sQXsMKgmGYgpOwqaSOxRXHtSAA\ngwUhYxCeSjg6YQXBMEzBMa5e5jPEIIJpihsCKc36WEXkG1YQDMMUHEVJjSFE44pjBhNg6ObKWUye\nwQqCYZiCkxBCtxyMWUwBhxoIgNeDGAlYQTAMU3DkmtSAuQ7CqQ8TYLMehIfyjVZYQTAMU3BMhXKG\nVhtOKa6A3XoQrCLyDSsIhmEKTkKxa/etOPZhApIKIcZrUnsGKwiGYQqOYnAxSUURjSsoSZfmyr2Y\nPIcVBMMwBUfYFcrF01sQAZ/VgmANkW9YQTAMU3CMLiYZbo4mRNosJmuaK68HkX9YQTAMU3CMLibj\nmtTpXEwphXKcx5R3WEEwDFNwhE2771hCuAxSswXhFawgGIYpOAljN1cyZjGlsSD8ZguCDYj8wwqC\nYZiCo9gUysUVgZI0FoRs1sd1EN7BCoJhmIKjFsqpA7wxLp0uSO23ZjF5J96ohRUEwzAFR7EplAOQ\nNgZhrYNgCyL/sIJgGKbgGF1MRlPAVaEcd3P1DFYQDMMUHHOrjSRuLAgulPMOVhAMwxQccyW10cXE\nrTYKCSsIhmEKjl0WE5DMVLLDb2nWxzGI/MMKgmGYgmOsgzAO9OnG/JQYhGfSjV5YQTAMU3CEQEqr\nDSD9oE9E8FGyUI4tiPzDCoJhmIKjCJHSagPIHHgO+Hx6qw3WD/mHFQTDMAVHzWJK1RCZBn1jHR0r\niPzDCoJhmIJjcjEZNEQmt5Gx0pq7ueYfzxQEET1ARJ1EtNGw7XYiaieiVu3nMsO+24hoOxFtIaIP\neSUXwzDFh2Jq1pfcnmnIN3Zw5W6u+cdLC+JBAJfYbP+FEOJk7ed5ACCiuQCuBTBPO+d/icjvoWwM\nwxQRCSUZgzBaDb4Mo76xToIL5fKPZwpCCPE6gCMuD78CwONCiIgQYheA7QAWeyUbwzDFhSKSg1F2\nFoTRHZV3sUY9hYhB3ExE6zUXVJ22bSKAvYZj9mnbGIYZBYics5iMNROsIfJNYISvdw+A/wQgtN93\nA/hcNh9ARDcAuAEAGhsb0dLSkpMg4XA453O9plhlY7myg+Vyz1A0ikRcoKWlBduOJvTte/fsQUvL\nAcfzYtEIAFWpePU3FeP9kngt24gqCCHEQfmaiO4D8Jz2th3AZMOhk7Rtdp9xL4B7AWDRokWiubk5\nJ1laWlqQ67leU6yysVzZwXK5x9/yT5QEgebmZlTtPgqsWA4AmDZtCpqbj3c8r3zlK+gaGgQRPPub\nivF+SbyWbURdTETUZHh7JQCZ4fQ3ANcSUSkRTQcwC8DKkZSNYZjCYW7Wl9yeKXVVprlyFbU3eGZB\nENFjAJoBjCWifQC+D6CZiE6G6mJqA/CvACCEeJeIngSwCUAcwE1CiITd5zIM8/4jIURyRbksAs92\nSoXJH54pCCHEdTabf5/m+DsA3OGVPAzDFC9qN1fVGjCN9S4L5ThA7Q1cSc0wTMFRHFxMGS0I7QBW\nD97ACoJhmILjtCZ15hhEqluKyR+sIBiGKTimBYNysSBYP3gCKwiGYQqKEMLkYjKSaeBnC8JbWEEw\nDFNQhLqcg20vpkzBZ7nsKKsHb2AFwTBMQVGEeclQUx2Ey/Ug2IDwBlYQDMMUlISmIHIplJPWBqe5\negMrCIZhCorVxWReMCj9uX49BuGFZAwrCIZhCop0Mdm2+84w8BNbEJ7CCoJhmIKSULQYhE3AOVN2\nkl8PbHshGcMKgmGYgqLph5z6Kvl1zcAawgtYQTAMU1BEShaTMQaRfuBPNvjzRLRRDysIhmEKinQx\nJVttJMlkTeh1EKwgPIEVBMMwBUWxZjFlYUH4uZLaU1hBMAxTUIQ1i8mwL3MWU+o5TP5gBcEwTEGR\nhXJkE6TO2GpDb9bHKsILWEEwDFNQUlxMpnbf6eEYhLewgmAYpqAoinOhnPssJtYQXsAKgmGYgqKk\n68WUsdWGu+OY3GAFwTBMQUm6mFLjCRkXDGILwlNYQTAMU1BSejEZ9mXs5sprUnsKKwiGYQqKoqTL\nYkp/LgepvYUVBMMwBSVtFlOGkT8Zt2AN4QWsIBiGKSgJSxaTz5TFlP5cH68H4SmsIBiGySv3v7ET\n025dgqFYwtXxiqVQDrm4mDgK4QmsIJj3Hd0D0UKLMKq5p2UHACAcibs6XljbfZsK5VwGqVk/eAIr\nCOZ9xaMr9uDkH76E7Z3hQosyalEs7bszkUhp953cl2ng5zWpvYUVBPO+4m/r2gEAnb1DBZZk9CKD\nzvJ35uPTtft2F6TmGIQ3sIJg3lcc6VfdS9VlwQJLMnqRA37CpYZIv2BQ+nP97GLyFFYQzPuKrjDH\nHwqNjCnEFcXV8QntsGRVdHJfxhgEB6k9hRUE876iS7MghEv3BpN/srUgrFlMxsHebasNxhsC6XYS\n0QYAjk9ZCDE/7xIxTB5QWEMUDHnv41kqCF0Z5NCsj5+3N6RVEAA+rP2+Sfv9sPb7U96IwzD5gQeM\nwiH1QjzhUkFoLib7LCZ3aa78uL0hrYIQQuwGACK6SAhximHXrUS0BsCtXgrHMNkgDKOE2wwaJv8I\n3YJwF4NIm8WU4VzpYuLH7Q1uYxBERGcZ3pyZxbkMMyL0GQqz2IIoHPLW5xyDMGUxZVhyVCoIft6e\nkMnFJPkcgD8QUY32vlvbxjBFw0Ak2dpBYROiYOQcg9Dem+sg0p/LMWpvyaggiMgHYKYQ4iSpIIQQ\nPZ5LxjBZkmAXU1GgZGtByBiEXvSWhQXBMQhPyegmEkIoAP5De93DyoEpVoxWA7scCo/rIHWaJUcz\nBSF0BcFRCE9wG0d4mYi+SUSTiahe/qQ7gYgeIKJOItpo2FZPRC8R0Tbtd522nYjoV0S0nYjWE9GC\nYfxNzCjFOGNlC6LwZB2DsNmXyYIgYgvCS9wqiE9ATXV9HcBq7WdVhnMeBHCJZdutAJYKIWYBWIpk\nFtSlAGZpPzcAuMelXAyjEzcpCB4xCo37LCb1d3JN6uS+TCEGGaTm5+0NroLUQojp2X6wEOJ1Ippm\n2XwFgGbt9UMAWgDcom3/o1D9Am8TUS0RNQkhOrK9LjN6UQQriGIiWwvCrt135hiE+puftje4zWIC\nEZ0AYC6AkNwmhPhjltdrNAz6BwA0aq8nAthrOG6fti1FQRDRDVCtDDQ2NqKlpSVLEVTC4XDO53pN\nscpW7HLt7UvOWNetXw90uP56e0Kx3y8AODyoIOgj1JTmPx2odf0G+A9uznjcxv1qevLgwABaWlpM\nluD69esQb/c7nrt1XwwAMNA/4Nm9LtbnCHgvm6v/ICL6PtSZ/1wAz0N1Cb0JIFsFoSOEEESUteIX\nQtwL4F4AWLRokWhubs7p+i0tLcj1XK8pVtmKXa6N7T3AsjcBAPPmnYjmuY0ZzhwZuYoNo1zTbl0C\nAGi76/L8XeAF9TOPnzMPzfObMh7evbYdWN+KyopyNDc3q5bHi88DAE4+6SScOXOs47ldq/cBG9eh\nrLzcs3tdrM8R8F42tzGIawBcAOCAEOKzAE4CUJP+FFsOElETAGi/O7Xt7QAmG46bpG1jGNewi6m4\ncN/NdRjrQUgXEz9vT3CrIAa1dNc4EVVDHdgnZzjHjr8BuF57fT2AZw3bP6NlM50OoIfjD0y2cBZT\ncZFrFlMuK8rx4/YGt07aVURUC+A+qBlMYQBvpTuBiB6D6pYaS0T7AHwfwF0AniSizwPYDeDj2uHP\nA7gMwHYAAwA+m92fwTDmAYlnlIXHbSV1yprUORTKscXoDW6zmG7UXv6WiF4AUC2EWJ/hnOscdl1g\nc6xAsmMsw+QEWxDFhVsLImHpxWTEtQXBz9sT3AapH4ZaA/GGEOI9b0VimNxIcAyiqMi2F5N9oVz6\nc1lBeIvbGMQDAJoA/A8R7SSip4joqx7KxTBZY4yJsoIoDEarIZHIrlDO3p3kzsXEeIMrBSGEeBXA\nHQC+CzUOsQjAlz2Ui2GyxmpBtB3uxwd/9io6e4cKKNXoImZQCq4tCMXZxZTZglB/c8zJG1wpCCJa\nCmAZ1JYbWwCcKoQ43kvBGCZbEgYTQlGAB5e3YXfXAJZs4IS4kSJqUBBZV1Lb7HO7ohzHnLzBrYtp\nPYAogBMAzAdwAhGVeSYVw+RAwuJiEml824w3xOI5WBB6L6bUfa5jEJzo6glus5j+HwAQURWAfwHw\nBwDjAZR6JhnDZIk5zTWZG59pFsrkj5ihxbf7NanNhXJGKFMMgoPUnuI2i+lmAOcAWAigDWrQ+g3v\nxGKY7LFWUlubwDHeEzO5mLJbk9o2RJ3JguBmfZ7itlAuBOC/AKwWQsQzHcwwhcBaB6EkTYjCCDQK\nieYSpLYUyhnhOojC4jaL6ecAggA+DQBE1EBEWbcAZxgvsVoQ8i2rh5EjNowgtX0Mwm2aK2sIL3Cb\nxfR9qOs23KZtCgJ4xCuhGCYXjD5vNUAtXUysIkaKWNwQg8g2zdVmH1sQhcVtFtOVAD4CoB8AhBD7\nAVR5JRTD5IKxDiKhCL1wjvXDyNEfTXqg3VsQ6m87F1Mm5a7XQbi6EpMtbhVEVBimZERU4Z1IDJMb\niiUGIVMfWT/Yo3hQPNDRM6i/dt3uO12QOsO53KzPW9wqiCeJ6HcAaonoiwBeBnC/d2IxTPZYK6n1\nGARrCFsSHgyq+7vVqvXqUMC1BSGEAJF9OnLGQjl2MXmK2zqInxPRRQB6AcwG8D0hxEueSsYwLjk6\npOAnL7yH8dX6argQwliAxRrCDrcDeDa0dw+ivqIEoYDPfR2EEHo9gxX3MQjWEF7getFeTSG8BABE\n5COiTwkh/uSZZAzjkvs2RLCpawc+bFjeUhGCXUwZ8MIts797EBNqQ+gZjLlv9604xxrcuphYPXhD\nWhcTEVUT0W1E9Gsiulhb8e1mADuRXOyHYQqK7O5gTLFUkklM7H5wwAsLYn/3ICbUlCHg8yGWhYvJ\n5zASuQ1Ss4bwhkwxiIehupQ2APgCgFcBfAzAR4UQV3gsG8NkhblQLllJ7YWv/f2AyxhyVuzvHsKE\n2jL4feS6kjqhCGcLImMlNVsQXpLJxTRDCHEiABDR/QA6AEwRQnD/ZKboMObdK0qyfZsXM+X3A/lW\nnEIIhCNxVIcCCPgoixiEs6WQsVCOYxCeksmCiMkXQogEgH2sHJhiQw4hxgFJEUnXEisIe/J9X+Tn\nBfw+BPyUVSV1rv2ykt1cGS/IZEGcRES92msCUKa9J6hLSVd7Kh3DuEBOMo1592qQWoUVhD35DlLH\ndQVB8Pt8WS056nPQEE7bk/uTn8Hkn7QKQgjhHylBGCZX7CwIYYhB8OBhj7k9uhh2OrBMEgj6fAj4\nsrUghpnFxI/YE9wWyjFM0RNXVFeF30emLCa3M9nRhrX77XCRClq1IMh1JfVwYhDsYvIWVhDMMY/P\n4GLy+wg+MtdBsIvJHuN9ycc9imkKIeDP0oJQnGMQbgvlWEN4AysI5n1DNK4qCCLVgpATWC96Dr0f\nsLYmGS7Sggj6pAWRBxdTRgWh/uYlR73BdSU1wxQrclnKoZiipT2q61HLQYNdTPYoebYgki4mX9Zp\nrn4HEyLjkqMcg/AUVhDMsY82hkTiCfh8BKGoA54c9DhIbY+pPXoe7pF0MQW1LCZjZXs6FEU4WgqZ\n0l+5UM5bWEEwxzxyDJEuJmiN+mIJjkGkwxSkzqcF4fMhmHUdhJOLyW03V37GXsAxCOaYh3QLQnUx\nkRaklgMUKwh7jElG+XDDSYsh4CcE/NnUQTi7mDJZELKSmh+xN7CCYIqeR1fswVOr9znul2NIJK7A\n5yPVzSSEPmCxgrDHFKTOhwWhfUbQTwj6yLWLKSGcXUyZYhBOTf6Y/MAuJqbo+dYzGwAAVy+clPa4\nhCK09Eo1i0kOWPe/uQu15UHcfP4sz2U9ljClueYli0mzIHxqqw23QWqhu5hSj6cMCoDXG/cW1r/M\nMY9xiPBRsg7C6OL4+YtbR16wIscYvM9LHYShUE51MbkNUsN5waAM5zqdx+QHVhDMMY9xjDDWQcRd\nujhGK+Yg9fA/L65nMfk0F5PLBYPSuJgyWQhyd1NNKO1xTG6wi4k55jEOIXoltSJcuzhGK0reXUwy\ni0mzIFwqaCGEcx1EBgOBiHDPpxbgpMm1WcnKuIMVBFPUZJu+6CPV7aC6mNiCSIepDiIP90pv1qe1\n+3a7oly6XkyZgtQAcOmJTRmPYXKDXUxMURN1MQs1ji0Bny/pYuLspbSYezEN//OM7b6DPvcWRGIY\nvZgYb2EFwQybaFzB9s4+Tz57IJIwvT/UF8HBXvOaVUYjQ01zVS0PdjGlJ/9BanMWk9oPK/PnplsP\nghVEYWEFwQybv7a249JfvoG+oVjmg7NkIGZWEOf9vAWn/XipaZtRD/h9qruCXUyZMU7w89qsz08I\n+tWhJebiGaSrpOY01sJSEAVBRG1EtIGIWololbatnoheIqJt2u+6QsjGZE9XOIpYQmAgmsh8cJYM\nROKm92HtfafBijCObX4iTUGALYgM5Lvdd9zS7htw9wwUxblimtVDYSmkBXGeEOJkIcQi7f2tAJYK\nIWYBWKq9Z44BInFVMUTj+Z+x91uUTtCvDhlvbj+sbzMGW32+ZKsNjkGkR8l3sz5Du++AZkG4UhBs\nQRQtxeRiugLAQ9rrhwB8tICyMFkwFFMVg5uAcrYMRM0WxAcaKgEAS9/rxCNv7wZg7sMjLQjBdRAZ\nybsFkUhaEFKRD9fFxPqhsBQqzVUAeJGIBIDfCSHuBdAohOjQ9h8A0Fgg2ZgsGdLiBG5772SDNUgt\nr7FkfQeWrO9Az2AMO3qS15V1EAlFuE6zHK3kO0htzGIK+LKxIJx7Kg13nWxmeBRKQZwthGgnonEA\nXiKi94w7hRBCUx4pENENAG4AgMbGRrS0tOQkQDgczvlcrylW2Zzk2rUnAgB4a8U7OFDjz+s1V+1P\nWhAtLS3oCQ+Y9v/sn1tM73t7ujEYAzoPDaZYECN9T4v9OW5sTyYVrFnbiqE9w3t2W3ZGAQBvLXsT\nOzrU5/bGsuVoKE/vqOjuHkRZgBAOx1PuVzHcv2J9joD3shVEQQgh2rXfnUT0DIDFAA4SUZMQooOI\nmgB0Opx7L4B7AWDRokWiubk5JxlaWlqQ67leU6yyOcn17MFWYF875p98ChZOrc/rNQ+s3AOsV5v1\nNTc3w7fsZQARx+PHjqkHhaOorw1B6TR/hUb6nhb7czy0ai+wYT0A4MT583HOrIZhfe6GxDZg61ac\n3/xBDKzfD2xch0WLT8P0sRVpz/vFxjdRW16CysqB5P16YQmAkX9mdhTrcwS8l23EYxBEVEFEVfI1\ngIsBbATwNwDXa4ddD+DZkZaNyQ3pYoqMQJA6U5zDR2odxMubbecXjIG810FIF5PP6GJyE4PIvO4D\nUxgKYUE0AnhG8y0GADwqhHiBiN4B8CQRfR7AbgAfL4BsTA4kYxD59/kPakFqOYBkypRSYxA82rjB\nmOWVnzoIRW+WqAepXWYxOfViYgrLiCsIIcROACfZbO8CcMFIy8MMH5nFFPPQgpDBymhcQW15EN0D\n9kV5PiJTYFNt/Z13sd4XGKuc81EzEtfW4wCQtCBcZDElFMHB6CKlmNJcmWOUIVkH4UEWU79WGJdQ\nBOIJBXFFoCzoHEwNaFlMAPCxhZPQVFOm78vnusXGFeuOVRJ5tiBiCUWvoA5kYUEIwes6FCusIJhh\no1sQHgyY4aFkFtOg5sqaUl/ueLzRxVQS8JkGvnxaEg+/vRuzvv0PHA47B8yLHePYnZdmfQmhK4ag\nP5sYhOClQ4sUfixMCv2ReFYDXyTmrpI6Ek+go2cwK1n6DK02BjV308XzxuM/r5hne7zPR/pstCTg\nM82S89mbSa6RvffIQIYji5e8rwehKLprSW+14UIrqwsGsQVRjLCCYFK45JevY9GPXnZ9vMxeyuRi\nuvnRtTjjzleycvUYLYiVbUcAAKUBH847fpzt8UE/6dW3KRZEHg0c2X3UqID2dw+i6xixKBKKwPr2\nHv29m66rmYglhB6clq023FiV7GIqXlhBMCnsPZLdLF/PYspgQby06SAAZNXUL2ywIG5+dC0AdeAP\nOcQhJtaW6QqiNOBPsSD2HR3AzkNh19d3Qg5oRvnOvOsVLMxCsRaSe1/fib+v26+/z0ffqnhCMbiY\nsmjWJ5zXg2AKCysIZthkm+bab+nQmo6wzbGlaRTElPpy3VIoDfhMcYeEInD2T17F+Xe/5vr6TkgL\nonfI/d9STGzq6DW9z4sFoQgEdRdTdllMnJpcnLCCYIbNkEsXk8Ru0Heiz2YALvH7UBqw/+pOHVOh\nN4gr8ftMA9/Db+12fd1MSAuiZzCGgWgcT7yzJ2+fPRKELPcvP2tSp1oQbrOYnBYMYgoLr0nNDItY\nQtHdOG7bffdHsnExxVBTFkTPYLLuoTTo07NkrEwdU677vUsCPtPAd/dLW11fNxOysKt3MIbv/vVd\nPLVmX94+eyQoDVoURF5cTCIZpPa7tyDYxVS8sAXBDIshw4pv6QKSxsC0WwsillAwFFML44yU+J3r\nIMZVlep+75KAz7M1IeTf2jsUw9aD3iy36iXWe5iXSmrFEKT2ubcg2MVUvLAFwTiiKM5rBUtkDQSQ\n3oIwpqu6jUHI42rLgjA6h0ps3Evjq0M4f844EJGuFEoDvrwWxxmRSq53MI4j/VFPruEl0YTZisvX\ninLScghmtWAQu5iKFbYgGEfcLPaSyYJ4besh/PDvm3C4L5n++Y0/r8Nyw4pwxvO/8thafUYu4w81\n5SWm4+wUxBWnTMCPrzwRQLI4y1oHYWS4ikMqr96hGI4OHHsKwurmy0uzvrgxzVXWQbCL6ViGFQTj\niJvZn1xuFACiNsdf/8BKPLBsF7Z3JlNLewZj+OT9K1KO3XtkAH9ftx8f/91bAAwKoszsYrILUBuD\n0dKtUeL3OVZPD7exYFgbYHsHY56sxe01VisuH21SIgkFJQHVdSWzmdw262MXU3HCCoJxxE2RUyYX\nU32FOvtfvqMr42fJWaxsxBc2uJiM2FkQRlGNQWonBmPDG9STFsSxmeZqVWrG55grsbiCEs1y8Ot1\nEC4sCI5BFC2sIBhH3MwqjRaEnUKRfZPe0aqgjVjdPMb1JK6+ZznCEVVRpAapbSwIw2fJGEQ6BREZ\nhoJIKEJXMHu6+nP+nEJiTBQoDfiGdT8k0YSi3/NsWm2o60GwgihGWEHkkWm3LsEdSzaZBs1jGTcu\nJuPMM53F8e7+3pRtB3qHTO+NCmLT/l7dxVRniUHYuZiMvm4pR2nAOdtpOBZEv7ZGxYVzxuGoQ9vx\nYmcgGsdy9yzhAAAgAElEQVTcpmr88XOLEQr6TbGkXDF2cw1m0WqDYxDFCyuIPCFnw/e9sQuzv/MC\nBqLHpuvBiDsXkyEGYeNi6h1yHkDf6zCnh0rFumhqHSLxhF77IN1UEruB387FVBrwYf6kGge5c3ep\nSPfSBXMa8eS/noHPnjXNtN+rzKl8MRATONIfxfFNVTj3uAaEgr68uJiicUW37tSFg9y32uAFg4oT\nVhB5whqM6x0cLQpCPaaixG/rkuodtFcQJX4flu8wZzJJC6K2vASKgJ75lOJiCshZanJQSRgsiITB\nxfTw50/Dty+bk3L9YVkQmoKoKA1g8fR6fO6s6ab9XtVe5Isblw7gcDiKihI1yz0U9A87JgOo3xej\nWy/o87nKhFMUcDfXIoUVRJ6wDo75KDwqNG4yUKQFURUKpigUIYSjojx1eh3e2GZWEFFdQagK4UDv\nEEoDPlSUmst15CC08lsX4t8/NBuA1YJIZjHVlAVxwsRUK2I4LhWZwVRZ6jfJY/07ihGjdSPvayiQ\nHxdTJK6YKtwDfsrCghj25RkP4MeSJ6yDQj6WcCw0riwIzS1UXRZIuQeRuGJSnETAmIoSlJf4cc6s\nBrx3oA+dhjiEtCDqdAURQXVZ0OR+WPntC/T3dRUlmFAbAmC2ICR6wNSfOjvNiwWhzcCtQfNIESsI\nY8sSqSxCQZ/eTysdkXjCtn5FEo0rpvhQfUUJXtx0AP9890Daz+U01+KFFUSesA6O1krVY5FsXEyq\nBWF1s5ndS6UBH5bfdj7WfPcinD1zLACYrAiZSVOrBaUP9gyhOhTQM2IAYFxVyPSZcmCx08dysLLz\nbw8na0cGz+UMPHgMWRAHe5MFizsPqxlYboPUd/3jPXzy/hXYaFhHwogxSA0Ad1x5IvYeGcS/Prwa\nh/rs18kQQkAR7GIqVlhB2LB+XzdW7Myct2/EOpgW8yzSLdm5mAIp90AGqBuqSgGoA1FpwI9Q0I+5\nTdUYU1GCN7Yd0o9PWhCqgjjQO4QaiwVhRTaHS2tB2JyfqwWxsb0HSzer61pUSgVhsVCKRUG8sLED\nrXu78czaZCPBgwaL7eoFkwCoz8WNwtx5SFUoT69pxyZLVlo8oUARZnfbB49r0KvbexxiUdLjxQsG\nFSfci8mGj/x6GQCg7a7LXZ9jVQjvBwXxm1e3Y0ZDRcqs3YgcWCpLAyl/c48Wf2isLsWhvghChuwj\nn49wypRabDZkMkUtLqaewRiqy4K6ErBDTljtWkWUpLEgcs3a+fD/vKm/1i0Ii3zFYD0KIfClR9bo\n70+dVo9nW/fr7rDX//08TBmj1qiky2I62DuEJes78NmzpiGkdYB9YNkuPLBsl+n/Q04mrF12m2rU\n745Tg0bpgrRzAzKFhy2IPJHiYnofKIg3th3GFx9alfaYIc3vXGbjppAWxPhqdZCwDtQ1ZSWmgUMq\nmBpD1lJ1KL0F4dctiNR9cjC0UzCDeWiPIS0Ia6M5+Xc8tzNasE6vVmX96pZD+Nk/t+CO5zcDAMZV\nl+r7QkG/Hkuy8m+PrcUPn9uEnYf709aVyO+7NWBfGVLvUdih4lzGcypLea5ajLCCyBPWLKZjUUFE\n4wpue3qDadu6ffb+ZkkklkAo6EdZSWqq5FGty2lTTRmA1MyuqlAAfYY6CVkHUVuWrHuoLgvYuogk\nZ80cg9l1Ptx66eyUfdKvbadghpvWSQR9Rm0lElcwGE3gL1tjuPbet4d1nVyxztj3dyeXka0IwrQi\nX7osJvk5/ZF4yt97y1/W6+dFHdqbyIFfVsVbkU0DrZlqTHHACiJPWP3vx6KCaNnSicdWpl8ZTQiB\nrz/Rin9s6ACgumpCQZ9toPNwWA1MynYb1szfytIAwpG4nk0T0QqtqkLJwSKTBVFeEsBtp5Vh5rgq\nx2PsFEy2QereoRi+YLCmRJrAajSu6D73fFgquWCdsW85kLRkplWb/+3TuZjKNEUyEE3orbwlT6za\ni2db2wEYFITFVSQVhN3KgEBSAcmUYcn/fmoBvvfhubbnMCMHq+08YZfieazhFJLu7B3COM1NFIkr\neHptO57b0IGm2jIMxVULQlUQCoQQ+sB5qC+C0oBPD1JbLYjKUACKUGfz5SVqmmxJwIeykuRgURUK\nZu2ffurLZ2Ld3m79fT4siL+17sfLWnA6E9G4gu5B1XoqhG/9pU0H8Y0nW03bjArinEnmwsN0hXLS\n0ugbiiNio0RkY0UnF5NU9k4xCNm2pKI0AKMEl53YZHs8M7KwBZEnck1z3XW4H7O+/Tx2HFLbYf/z\n3QO4aWl/XgqXssWptm/v0aR7QnYBjcYV3PjIagxEEwgF/PpM06gYD4ejaKgq1Qd868fr7gdtdhmJ\nJ1Aa8KHcoCDG15Rm3YZh4dQ6fO7sZHWz3fnZVrqXBZ3971aicQU92sDptDSql/zrw6tSusy2ay6m\nr104C6c2mv+W0qAf0bhiapkukQqieyCKwVjqPZNxJmlBW//eitL0MYhwxJwyzBQXrCDSkM0iKlaL\nwa2L6Zm17YglBJ5dq5rqdz6/Gf0xs8+40EhXEZAMKs5urML+niF0hSOai0n9KhldKofDEYytLNUH\nV6sCkrNLudpcJKYGvI3ZTlPqK9JmMbnB6mKqLQ9mvchPus6wViJxBd2aiyld/MQrnIrOTphYja9d\neFyKwpTPzs7qlft6BmO27jKpaHULwqIggn71u+FkQUjFwUHq4oQVhIHBaMI0i8pmFp9rkFpeT2bC\nyN/n3/0avvr4WtfX9xKpIBQh9OU1ZRZMV38UpcGkBXF0IKrHFA71aQpCtwgsLqYUC0JBadBvygqa\nOqZ82I3crOePrw7prhG3ZOMyvOnRNWjVXFyFsCCcFESjQ7qyVMh233c54Pc4LIwkYy3y+28tGgTU\n59zn5GJiC6KoYQWhMRRLYM73XsDdL23Rt2UzKMRyjEEktMFUFgoZ/7mfbd3v+vr5wGl5yMN9qlJ4\neXccV/xGrRGRcYUj4agegwBUxfb4O3vV88IR1cXkYEEkM1ySs1BrK+8xFSUphWjZYrVAxlWHsrYg\nBrPszrtMa0lRiBiEU83Z+BoHBaE9H7tUVznwdw/EbBVIZ59aeCcnRKU2CrGyNJDRxVRZwgqiGGEF\nobGq7SgANcAnGY4FYacgwpE4OnrMrqMUC6KA9UJ2QciasiAOhdVBoKM/uV8Wz/VF4ggFfKa0yX++\newB7j6gdQxsqS3QLwi5IDaiLCSUUgUg8keLKISLdAsj13vgNg/RzXzkbE2pCWa/jkO2yoru0NhbD\nfZxDsYQen3KLk8V18bzxttvLSnzatVKfv/weSwti8fR6XDS3Ud/fqbXuSLeKX2Uo4Byk1tNc3cd4\nmJGDFYTGG9vVlg8nTarVtxkVRDyhYO2eo47nuymUu+ae5TjjzldM22ScIzkIFk5D2M0gG6pKdQui\nP5Yc4MdVmQutjDnyQb8Pn3vwHQDA5PpyPehsjehUlarZNP/98jZ85bE1qovJMMBIK0VaAFecPDGn\nv8sYBzhhYg1qy0vQbXCF2bH3yAAO9CTbUlgVxBkzxqS9pkzrlOdtOdDn2G4iHQ8tb8MFd79mmrhY\nURSB1buT302n79A5Wv8rK+lcTFJBdA/GMBhLYFJdGeZNqNb3t3cPIhJP6N93O5daOguiP6rWV1hT\naJniYNQ/lYQisHTzQazR/sGMsy+jFfCTF97Dlf+73LEyNjWLKVVBvHcg9VzpYpKKolAKYndXP1r3\ndKdsH1tZoscgjAqiwaQgfKYsnxK/D92DMZwwsRpXLZikV+Bas2QqDfUOz284gC0H+vRj133/YrR8\nsxmA+kxWf+dC/PSa+Tn9bdYZdX1FEHFFOM5qAeCcn76K0+9cqr83poH+5Utn4A+fPdX2vJe/fq5e\n9wEkfewf+u/X8bHfLnct845DYew4FEaHpqS+9+xGx/WdH1zehqvvWW7qa2XkguPHofV7F6VUfEvk\ns1yyvgNCqP8P8lnJehEZpC7T+mlJInEFa3Z3O6a5AkBlaRAr245g75GBlH3hSJwD1EXMqFcQD7y5\nC59/aBXe0VxM/YaZonFG9ZbWvK8/Q08ZiZ27RiLN8fX7uvV8fZkhYk3YsUs9zDdCCHzwZy348+p9\nKfsaqkIGBWHcbrUgkoNG0E8YiiZw6rR6+H2kuw++cM4M02db3Qpd/VHdgqgpC5oCl2MqS3MO+Fob\nwclusXaB6kg8YWpuJzGuEDiprtz09xqZVFeOSXVlyfNiCf3Zbj3o3lV0wd2v4YK7X9O/Vx09Q3jl\nvU7bY7d1qp/bdrgfiiL02gJJXUWJ/jfbsXBqHa44eQLueW0H/rxqHz7/0Co8smI3gOQkaeehMI4M\nRFFekrQWZafdN7YdSgapbZ7RjIYKAMAvl25L2dcfiXOAuogZ9Qpig6V1sbH1w1BMQevebtyxZBM2\ntqvdK50WAkpXByGEwKMrkhXK0u3wkV8vwxpt1i5nqNbBLJNb4u/r9tvOzLJh7d5Uy0Fy/PgqtHUN\n4IWNHY4upvKSgKm4Lej3YSCW0NdLKA340XbX5fi3C2aZPts4E52qNY7LJp3ULXLmLBcXkt1i7QLV\nv3x5G/7fE+v097u7+vGnFbtNLibrCndGAj4yWSxCAP/bst21rJF4An9Ytkt/3zsYw5T6clSFAnjd\nwUKQQfxoQqAvEk9JBnBqCSIhInx4/gQkFIFl2ip/0tqNxBWtJYr6uWWGyUBVKIh5E2vQujdpQdit\nF37bpcdjSn25KV1a0h+J698TpvgY9U/Gumay0Vc6FE/g/jd2Yqlh5uYUrEyX5rp+Xw++9Uyyx1F/\nJI7qkPnWD8YSeHzlnpRZZld/FHUV9rM/RRH4ymNrMbayFKu+c6HtMYCqoH7/5i5cPr9J74tk5O00\nrc2/eM4MPPHOXjy5ah8G4smRZ0xlUkGMrSwxuZgSikBCESh3EXhcNLUO588Zh76hOO5p2YE9w1R2\nThg7j8pusXaB6u2d5vv/rWc2YNl29f7MHFeJl7/+wbTX8fsIVy2YiK5wFLMrBvHM9hj+55WkgugZ\niOnNCFv3dmN3V78ptvL7N3fhpy8kM+mOaM8/FPQ5rqkgZ+1DsYTtEq+hNE32JHLtbllxLS3lSDyB\nc49rwOq2ozjQO4SykoCuBIiAyXVleHd/r2M3V/U4wtQx5bb3m11Mxc2otyCMC6gA5p4xkZiCtq5+\n035HBZEmSC1rB5KfEUeXZdtgNIHv/HVjSssD67mmc7Rj7WZmRvYcGcCPlmzGjX9aY7u/K+x8jZKA\nDxNry9A7GIPx/7vCYDE0VJWi1DBLPaLNzMtdVB//5ctn4sbmmfjahbNw4ZxGfPr0qRnPGS5SuR22\nGXCtK6ut2HlEf22s8HaCiHDlKZPw/FfPwdiyVJ//ts5kHOqjv1mGrz5ubolhdXttPRhGdSiAsZWl\nWLOnG3f+Y3NKAad0hR7qi+gW58xxlXpwvsyF3I3VITRWl+qWg16bElMQCvixeHo9ALW9urQgiICJ\ntWV6oBpwtgDrtMSAlbuO4EGDhdQzGEd1GSuIYmVUK4iEIrDTkkJotCgGY3FTmwnAuflaugWDrG6i\n/kgiZabc2Tdku9j9kX7nwd/qa3ZCKjWj8lu9+wiuu/dtPLhsV1olBKhFTB09Q6YsJGPWibFaWpVZ\nUxBZzAxLA37cf/0iXLt4iutzckW6xzptFMQey4TA+EyyabcBpDauA5ItL5ywnnE4rC67OrZSXVPj\nd6/txLp9ZpegrNo+FI7oFsSPPnoCfvTREwDAMV5iZZah4aH8P1CLF304VVMQ7UcHdZcVgTChtgzR\nuIIf/H0TgHQKIoij/VF8/Hdv4XbtWED9ftc7WMhM4Sk6BUFElxDRFiLaTkS3enmtowNR00DeVBMy\nDaK7uwYQjStYMCWZ+vrj5zfjiXdSO546WRDPtrbjG39eZ9rXH4ljT5dZQXR0D8EOq4VjxG2nUDkr\nNaZ7vry5E2/t7MIjK/akWDMAcO5xDfj99YsAqC0x0g1sDVWlpkFIWiTF6luuKA2gqjRgWl0NUGfi\ndi6uuU3VKdvcMBhPVfh2biJjMoSdsq4OBU1JAe919GF7Zx+++MdVJrfSYYMFUVMW1J+JXVzADuM1\nZPaU7I/1sYWT8IlFk/GFc2boLiui5IJAEqeixtryElN/KEUREFplvpMLlSk8RaUgiMgP4DcALgUw\nF8B1RORZz99uS5ByUl2ZKfXxv19Wsy4WT0/mvHf2RXDLU+Y1EwDnNNevPt6a4hLojyaw26Ig9vek\nDsABH6X0ZPresxtx3+s71c+JuFUQ6t9p9A/LYHxH96CtlfKlc2fggjlqQVQmH/FYS4aRdHm5cckU\ninHVpXoVsGTvkQEYH9UXzp6Oq06ZiOsWTwZgn/UkeerLZ+I/r5hn2nba+AAun2/uSnrIxh1ojBsc\ntFEg1WWqi0myob0b33hyHV7adBAb23t0uQ6Hkwqiuiyoz/TdWhBjK5MD9b6jgzjzzqXoG4rrWWo/\nuWY+JteX6+5EAjCh1hzTsvZiktRZAvt9kTj6InHEEgJjWEEULcU2xVsMYLsQYicAENHjAK4AsCnt\nWTliDZqpX/bUYrhrT52MsqAfv3h5q76tvXsQEw3/HJEs0lwHoqkV1XZVrJPqVP9ufySO03+8FHdd\nPR9/fEtNP/ziuTNMqZfpkH+nse2DtJT6ownsOtSP+ooS0+zVmIlTGUr/NaktM//zS6usmBVEY3UI\nB3qGTO3JpdK+66oT4SPCx09VFcM7bWocQrbwtmPh1DosnFpn2lYaIPzyEydjyXp17Yzx1SG96NA4\noegdiunt1A/2pFqSNWVB0+C9fl+ProQFki7MQ30R3TVUUxZEqaYY3CuIpBK6bvFk/H1dh/Z3mAf9\nUt2CINP/wA8+Ms9xjQyrldA7GNMnTvUVpXanMEVAUVkQACYC2Gt4v0/b5glyxbN7PrUAv/7kKbYz\n5e9cPgdTx5TjqxeaUzQ/ed/bOO47/9Crca0WxMq2I7inZYftdfsjCRzsHcIHtPxwJybUlmF/9yC2\ndYbRF4njpkfNQWZjwHzarUscGwTKdM6gz6fXVRhdaf3RhKm4CzDHGIz35SdXn4invnym6VinAqzy\nInUxAaqCWLOnG1ffs1y3pnZr7qWL543XlQOQnCVn254DMN/HhqpSfWDvMlhtPYNx/ObV7Zj3vRdw\nwOD2Ommy6tqsDgVRbVDC2zrDuuuxbyimK4jeoTj2HR1Ua09K/LorKFOaq1E+QF1D/M6r5mP6WPX7\naV1qVM9igjrw/+ya+XjrtvNx/ZnTHD/bWofx0PI2NP+8BQDYgihiivc/2AEiugHADQDQ2NiIlpaW\nnD4nHA5jzT7VVRTeuxkN5T4cOpBq3s9M7MFrr6XGHORs8+8vtqC6lHCwM3Xm98uX3gMhtcXEuk1b\nsGN/HHUhwtcXluLFtjg2dqW6i3xDPdh1OIE3V5jXhfYT0NLSglUHzBbE8y+3oDak/vN2Dyn4Wssg\nPndCCXb2qIpjZdsRzPjW8/jDh8qx7+AQSvyA1DGhuLnKe13rGvTtUgeGjj3JgbGqewf6+gktyUQU\n/RnctjiEp7ZFsfWoer2NravRtd37OUg4HM76e3DkkPqs1+zpxjf/8Aqum1OK5ZsiKAsA61YuM82E\n5Ux3Yjmyuo5VLl80jLY+gZaWFrT1JJ/3spWr8V+r5XcvgQumBHBSgx8rOtQEii1bt6K/Ur2PtaWE\n7khyIvD2mg3oHYxhUiVhX1hg2aa9KPMLvPbaa9jTq16jbesmtBxJWr9O92v/Yc2qHIygpaUFFFG/\n03vbdqKlJTlva+9Trz8wOICWlhY0ANiydge2pHxikl095u/3/W8mv0C7tmwAHfDn9BxHgmKVC/Be\ntmJTEO0AJhveT9K26Qgh7gVwLwAsWrRINDc353ShlpYWNFZNBja+h0svOBeVpQGsHHoPL+5Ozvon\n15fB9PkvLEn5nIaZ83HCxGocWfEGANVtdP0ZU3E4HMUSbVnOT542Ba9vPYR9WkYUVTeib+8BnDV9\nPP7tqvnoeHoDNnYlldAZE/y4/eNnYcmGDix7ZRvGTjkOWJWMeyQEcPpZ56BrfQfQmgyAzzvlVMxq\nVDNRlu84DLSswAMbU90icxeeAd+6lZg7wa+3pV48Zzre7kjm65926iLMm6Dmxh9Zsw+PbFavc+mF\nzfrg+dqJ/YglFH25z2YAu5/ZgK1aUeB555zp2EE0n7S0tCDb78Ga2Fa8tm8bKkr82B+vQHPz2Xhw\n10rMGBfBeeedk3L80zOPYtqYiqwybnS5tO/N7KlNeG3rITQ3N2txpM0AgFDjdMxubMcWrY3LN688\nA3OaqvFsazuWPd6KU0+ah4+cNAHHzetEVWkA1/z2Lf0aoYYpENiGM4+fiCdX7cOesMDE2nI0NzdD\nCIEpsw/hnFkNJpeh0/0at78XP1/1BgT50dzcjL93rsP6w/sw5/jj0GxIPz7QM4RvL1uKc+dMRHPz\nSa7uxfE9Q/jBW0tt9110zhmYXF+e03McCYpVLsB72YpNQbwDYBYRTYeqGK4F8EmvLnZ0IIagn/Sc\nfqOv9qdXz8eHTrDvfmlke2cfXt3SiY6eIXzytCl4dMUezBxXiY8tqtMVxPmzx2FHZ1hXELIdtuyI\nak2frA4SZo+vwrv7eyAE8JAWdzByqC+CAUvNhDGdNt16FNsOhtE3FMfcpmpdQRw33rymsykGYXAx\nGWfWU8ekusiMf4ub/PtCcWPzB3DhnHFYsqED972+E39d247Vu4/ivNnjbI9fMKXOdrsbVnzrAsQS\nCh5dsQdd4Sh2HArjjuc36/t//Px7+uuKEj/maFlTV5w8EZPqyvUsuvNmj0sp7JRN/D56sqogYgmB\nGs0dRURodvh77BhbpSo/mWAxRot79FmuOb4mhOe+cjZmNVa6/uzxNSE8+sXTsLtrALc9bU7yGFPJ\nLqZipahiEEKIOICbAfwT6vTqSSHEu15dr3sgitryEn3QMwbj5k6o1v/RrBhd7i9v7sQT7+zFJfPG\n44PHNQBQs4WMHS8bq0O2LZjl7Fq2W5ZEtbH98vlNOGlyLTZ39Kacu+twP1a1HTFtMyqIdOsdbOvs\nQ+9QDNVlQSz9xgfx5y+dYWrhDJjXbsgUpDZi9HcXc5A6FPRj/qRanDSpFooAvvZEKwjqkpz5prE6\nhEl15WisDiGuCDy5Sp0g/Oq6U0zHnTa9Hm/ecr5p28KpdSalXB1Sv5Nzm6oR9BM2d/SitjyI02aM\n0X351Q7f20yM0YLFH9Uqu+Xn2RVSnjCxJiU2kYkzPzDWtNa0TB8u5ljVaKeoFAQACCGeF0IcJ4T4\ngBDiDi+vdXQginpD8MzccM751sxoSM6cXtt6CD2DMVx/5jQ9Bzzo94GI8M2LjwOguqrsFIT8Z5ft\nL2QB1wljZf66H/9yprmyeLbmQvrMAytTFhRa+l6nntufrjp668E+hCNxVIUC+EBDJU6dVp/yz25M\nzZVtud1QZqiyLcRqatkiM4+uPGUiWv79PNOzzTdztUnD717biRljK/CRkyaY9h/XWOWqJqD1exfh\n6RvP1C2706arTRFlU7xcFYTfR1j73Yvw46tOBACcPUttD356htbm2VBlsEafvvFMrP3uRXn7bCb/\njGrVfXQgZmq8Zpz9pmsaN31sBbZ3hvGV82fi6gWTUFbiR2N1SG+3LNNJbz5/Fj571nRUlAZs0//k\ngPGp06bgg8c16Fkkby97Qz/mLEsP/+PGV+m+aslbt52PM+58BY+u2IMX3z2IVd+50JSy+rGFkzC+\nJqT3BGrZcghCJNeEtsOoIGTXVTfJMHobhsyHFgWN1SFsuP1iVIVyG1SzwWhVfnB2Q8p+a62AEzIj\nqLwkgKMDMcwcpyq1M2aMwTttR4d1740Kat6EGmz+4SV5dRUaM96sXYCZ4qP4p3gesbkrgTW7j5r8\nqNaW1U7I9L+mmjJMG1uBRi2HXS5sYywWkq2M5cfd/5lF2HXnZdh152X65xARJteX2/7DjKsK4XhD\nfGB2Y2WKNTLOsNawTKM0KoimmpBem3BcY6VeJWsdFNvuuhy/+eQCAMA0Q3xBugDcjBNSEY6v9j44\nnS9GQjkAZlfKJ7Q02lMMVfrpWnLbIYs6ZYry2bNUpbNpf6pLMleKOY7EeM+oVBB7jwzgN61DmDqm\nHP9xyfH6dmMMwqkiFEi6gqwZLSWBpIvJys3nz0RFiR8LNJ+yU0GRHS987VxcvWASALWYSbbGllgV\nxtLNB/VAuJRTduf82MLJutVkZ0FcPr8JbXddrnccledPqAnhX+ZlLmhaMKUWH2iowB8+u9jlXze6\n+PcPzcZHTpqA48er1sQzN56Fs2aqLpy6iuwUlYw5TalXlfkpU2oxc1wlbrtsTh4lzj+XnjAeXzxn\neqHFYFwwKl1M2zvDCPgI919/qh4HAKBXngL2Lqa7rjoRy3Z06YrBmn0xs6EKZ8wYg3kTU3v3LJxa\nj3d/eEnOMieU5IIs08dUYOehfsdjP/+QuW6ivrIUX5w9Duv2deOqBRPR2TeE+97Y5XrmXBLwYflt\nF7jKtz5lSh2WfqPZ1eeORm46b2bKNjkZyTVYKycMQb8vYzvyYuCe/7uw0CIwLhmVCuK848fhp+eW\n6S4eibFvvp0VcO3iKbh28RR09AziknnjU5q41ZQH8dgNp3sis/TdBgO+rIOQFSV+TBtbgee+oub3\nf+7s6WjrGsCJE2vyLieTPZWaok63RnY6jiV3HnNsMSoVBGDfinnuhGp8aF4j6itK06ZoNtWU4bef\nHtlZ0K2XHo8Svw8Xz23Eu9oqeIum1uE6S3vsGQ2qdXHucQ14Z9cRDMYSKcquqaYM931m0YjJzqTn\nu5fPQUWJP6uaBQB45POnYX17t2OrE4YZLqNWQdhRUxbE7z5dnAPnuKoQ7rp6PgDgpvNnIqEIfOPi\n2XoQ8Q//cip6h2I4aVItHntnD/7jQ8fjSH8UDyzblZIJxRQX46qTzzYbzp41Vk9FZRgvYAVxDFId\nCoNXkdQAAAeUSURBVOI7HzZ3QT/v+OTs87ZL1SBlQ1UpbjEE4RmGYbJhVGYxMQzDMJlhBcEwDMPY\nwgqCYRiGsYUVBMMwDGMLKwiGYRjGFlYQDMMwjC2sIBiGYRhbWEEwDMMwtlCu/V+KASI6BCB1PU53\njAVwOI/i5JNilY3lyg6WKztYruzJVbapQojURUksHNMKYjgQ0SohRFH21ShW2Viu7GC5soPlyh6v\nZWMXE8MwDGMLKwiGYRjGltGsIO4ttABpKFbZWK7sYLmyg+XKHk9lG7UxCIZhGCY9o9mCYBiGYdIw\nKhUEEV1CRFuIaDsR3VpgWdqIaAMRtRLRKm1bPRG9RETbtN91IyDHA0TUSUQbDdts5SCVX2n3bz0R\nLRhhuW4nonbtnrUS0WWGfbdpcm0hog95KNdkInqViDYR0btE9FVte0HvWRq5iuGehYhoJRGt02T7\ngbZ9OhGt0GR4gohKtO2l2vvt2v5pIyzXg0S0y3DPTta2j9j3X7uen4jWEtFz2vuRu19CiFH1A8AP\nYAeAGQBKAKwDMLeA8rQBGGvZ9lMAt2qvbwXwkxGQ41wACwBszCQHgMsA/AMAATgdwIoRlut2AN+0\nOXau9jxLAUzXnrPfI7maACzQXlcB2Kpdv6D3LI1cxXDPCECl9joIYIV2L54EcK22/bcAvqy9vhHA\nb7XX1wJ4YoTlehDANTbHj9j3X7ve1wE8CuA57f2I3a/RaEEsBrBdCLFTCBEF8DiAKwosk5UrADyk\nvX4IwEe9vqAQ4nUAR1zKcQWAPwqVtwHUElHTCMrlxBUAHhdCRIQQuwBsh/q8vZCrQwixRnvdB2Az\ngIko8D1LI5cTI3nPhBAirL0Naj8CwPkA/qJtt94zeS//AuACIsr7Atxp5HJixL7/RDQJwOUA7tfe\nE0bwfo1GBTERwF7D+31I/w/kNQLAi0S0mohu0LY1CiE6tNcHADQWRjRHOYrhHt6smfcPGFxwBZFL\nM+VPgTrzLJp7ZpELKIJ7prlLWgF0AngJqsXSLYSI21xfl03b3wNgzEjIJYSQ9+wO7Z79gohKrXLZ\nyJxv/hvAfwBQtPdjMIL3azQqiGLjbCHEAgCXAriJiM417hSqvVjwVLNikUPjHgAfAHAygA4AdxdK\nECKqBPAUgK8JIXqN+wp5z2zkKop7JoRICCFOBjAJqqVSFIumW+UiohMA3AZVvlMB1AO4ZSRlIqIP\nA+gUQqweyesaGY0Koh3AZMP7Sdq2giCEaNd+dwJ4Buo/zUFpsmq/OwsknpMcBb2HQoiD2j+0AuA+\nJF0iIyoXEQWhDsJ/EkI8rW0u+D2zk6tY7plECNEN4FUAZ0B10QRsrq/Lpu2vAdA1QnJdornrhBAi\nAuAPGPl7dhaAjxBRG1RX+PkAfokRvF+jUUG8A2CWlglQAjWY87dCCEJEFURUJV8DuBjARk2e67XD\nrgfwbCHkSyPH3wB8RsvmOB1Aj8Gt4jkWf++VUO+ZlOtaLZtjOoBZAFZ6JAMB+D2AzUKI/zLsKug9\nc5KrSO5ZAxHVaq/LAFwENUbyKoBrtMOs90zey2sAvKJZZSMh13sGRU9Q/fzGe+b5sxRC3CaEmCSE\nmAZ1nHpFCPEpjOT9Gm6U+1j8gZqFsBWq//PbBZRjBtQMknUA3pWyQPUbLgWwDcDLAOpHQJbHoLoe\nYlD9mp93kgNq9sZvtPu3AcCiEZbrYe2667V/iibD8d/W5NoC4FIP5TobqvtoPYBW7eeyQt+zNHIV\nwz2bD2CtJsNGAN8z/B+shBog/zOAUm17SHu/Xds/Y4TlekW7ZxsBPIJkptOIff8NMjYjmcU0YveL\nK6kZhmEYW0aji4lhGIZxASsIhmEYxhZWEAzDMIwtrCAYhmEYW1hBMAzDMLawgmAYA0SUMHTvbKUM\n3X6J6EtE9Jk8XLeNiMYO93MYJp9wmivDGCCisBCisgDXbYOaT394pK/NME6wBcEwLtBm+D8lde2O\nlUQ0U9t+OxF9U3v9b6Suw7CeiB7XttUT0V+1bW8T0Xxt+xgiepHU9Qfuh1p8Ja/1f7VrtBLR74jI\nX4A/mWFYQTCMhTKLi+kThn09QogTAfwaapdNK7cCOEUIMR/Al7RtPwCwVtv2LQB/1LZ/H8CbQoh5\nUHtwTQEAIpoD4BMAzhJq87gEgE/l909kGHcEMh/CMKOKQW1gtuMxw+9f2OxfD+BPRPRXAH/Vtp0N\n4GoAEEK8olkO1VAXQrpK276EiI5qx18AYCGAd7RW/mUoXLNGZpTDCoJh3CMcXksuhzrw/x8A3yai\nE3O4BgF4SAhxWw7nMkxeYRcTw7jnE4bfbxl3EJEPwGQhxKtQ1w2oAVAJ4A1oLiIiagZwWKjrM7wO\n4JPa9ksByAV8lgK4hojGafvqiWiqh38TwzjCFgTDmCnTVhaTvCCEkKmudUS0HkAEwHWW8/wAHiGi\nGqhWwK+EEN1EdDuAB7TzBpBsx/wDAI8R0bsAlgPYAwBCiE1E9B2oqwz6oHaxvQnA7nz/oQyTCU5z\nZRgXcBoqMxphFxPDMAxjC1sQDMMwjC1sQTAMwzC2sIJgGIZhbGEFwTAMw9jCCoJhGIaxhRUEwzAM\nYwsrCIZhGMaW/w9UCD+yiqtWJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f34704cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    \n",
    "    ax.set(xlabel='Episode', ylabel='Reward', title=name)\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(\"%s.png\" % name)\n",
    "    plt.show()\n",
    "    \n",
    "plot(range(episode), plot_history_episode_rewards, 'Actor-Critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a4d46fc59502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projs/NTHU-ELSALAB-RL-2017/Lab3/cartpole_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py3/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    rollout = do_step(env=env, policy=policy)\n",
    "    agent.train(rollout=rollout)\n",
    "    env.render()\n",
    "    if rollout[0].done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
