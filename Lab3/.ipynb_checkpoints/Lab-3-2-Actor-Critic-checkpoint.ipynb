{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3-2: Actor-Critic\n",
    "    In this lab, you need to implement a REINFORCE algorithm with Tensorflow and solve OpenAI Gym CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole_env import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Define the data structure of experience\n",
    "Experience = namedtuple('Experience', 'state action reward next_state done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```discount``` function to compute discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards, gamma):\n",
    "    '''\n",
    "    param rewards: a rewards numpy array\n",
    "    param gamma: discount factor\n",
    "    '''\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    # TODOï¼š Calculate discounted rewards\n",
    "    discounted_sum = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        discounted_sum = discounted_sum * gamma + rewards[t]\n",
    "        discounted_rewards[t] = discounted_sum\n",
    "        \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```do_step``` function to collect step results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_step(env, policy):\n",
    "    '''\n",
    "    Collect a step from env with policy\n",
    "    \n",
    "    param env: RL Environment\n",
    "    param policy: a function parameterized by environment state, return a action\n",
    "    return a list (state, action, reward, next_state, done) with length 1\n",
    "    '''        \n",
    "    # Empty list\n",
    "    rollout = []\n",
    "    state = env.current_state()\n",
    "    action = policy(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    rollout.append(Experience(state, action, reward, next_state, done))\n",
    "    state = next_state\n",
    "        \n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```ActorCriticAgent``` following ```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent(object):\n",
    "    def __init__(self, sess, n_states, n_actions, n_hiddens, lr_a, lr_c, gamma):\n",
    "        '''\n",
    "        param sess: tf session\n",
    "        param n_states: dim of states\n",
    "        param n_actions: dim of actions space\n",
    "        param n_hiddens: dim of hidden state\n",
    "        param lr_a: learning rate of actor\n",
    "        param lr_c: learning rate of critic\n",
    "        param gamma: discount factor\n",
    "        '''\n",
    "        self.sess = sess\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr_a = lr_a\n",
    "        self.lr_c = lr_c\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "       \n",
    "        self.state = tf.placeholder(shape=[None, n_states], dtype=tf.float32)\n",
    "        self.value = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        # For value loss\n",
    "        self.td_target = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # For policy loss\n",
    "        self.td_error_in = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    " \n",
    "        # TODO: Actor (using policy function)\n",
    "        self.policy = self.policy_function(self.state, n_hiddens, n_actions)\n",
    "    \n",
    "        # TODO: Critic (using state-value function)\n",
    "        self.state_value = self.state_value_function(self.state, n_hiddens)\n",
    "        \n",
    "        # TODO: TD-error\n",
    "        self.td_error_out = self.td_target - self.state_value\n",
    "        \n",
    "        # TODO: State Value loss\n",
    "        neg_log_state_value = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self.td_target, logits=self.state_value)\n",
    "        self.value_loss = tf.reduce_mean(self.td_error_out * neg_log_state_value)\n",
    "        self.train_op_critic = tf.train.AdamOptimizer(learning_rate=self.lr_c).minimize(self.value_loss)\n",
    "\n",
    "        # TODO: Policy loss\n",
    "        neg_log_policy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=self.action, logits=self.policy)\n",
    "        self.policy_loss = tf.reduce_mean(self.td_error_in * neg_log_policy)\n",
    "        self.train_op_actor = tf.train.AdamOptimizer(learning_rate=self.lr_a).minimize(self.policy_loss) \n",
    "\n",
    "    def policy_function(self, states, n_hiddens, n_actions):\n",
    "        '''\n",
    "        Define policy function using Neural Network to implement\n",
    "\n",
    "        input:\n",
    "            @param state : input state\n",
    "            @param n_hiddens : num of hidden units in neural network\n",
    "            @param n_acion : dim of action space\n",
    "\n",
    "        output:\n",
    "            @return policy : the probability how to choose an action\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        h = tf.layers.dense(\n",
    "            inputs=states,\n",
    "            units=n_hiddens, \n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.1),\n",
    "            bias_initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        policy = tf.layers.dense(\n",
    "            inputs=h,\n",
    "            units=n_actions,\n",
    "            activation=tf.nn.softmax,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.1),\n",
    "            bias_initializer= tf.constant_initializer(0.1),\n",
    "            name='action')\n",
    "        \n",
    "        return policy\n",
    "        \n",
    "    def state_value_function(self, states, n_hiddens):\n",
    "        '''\n",
    "        Define state-value function using Neural Network to implement\n",
    "        \n",
    "        input:\n",
    "            @param state : input state\n",
    "            @param n_hiddens : num of hidden unit\n",
    "        \n",
    "        output:\n",
    "            @return value : value computed by state-value function\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        h = tf.layers.dense(\n",
    "            inputs=states,\n",
    "            units=n_hiddens,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.1),\n",
    "            bias_initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        value = tf.layers.dense(\n",
    "            inputs=h,\n",
    "            units=1,\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.1),\n",
    "            bias_initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        return value\n",
    "        \n",
    "    def act(self, s):\n",
    "        '''\n",
    "        @param  s: a np.ndarray with shape [n_batches, n_states]\n",
    "        @return    a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Softmax stochastic policy\n",
    "        probs= self.sess.run(self.policy, feed_dict={self.state: s})\n",
    "        action = np.random.choice(self.n_actions, 1, p=probs.reshape(-1))\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    \n",
    "    def estimate(self, s):\n",
    "        '''\n",
    "        param s: a np.ndarray with shape [n_batches, n_states]\n",
    "        return a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Critic output\n",
    "        value = self.sess.run(self.state_value, feed_dict={self.state: s})\n",
    "        \n",
    "        return value\n",
    "        \n",
    "    \n",
    "    def train(self, rollout):\n",
    "        '''\n",
    "        param rollout: a list of experience\n",
    "        '''\n",
    "        states = np.array([ np.asarray(e.state) for e in rollout ])\n",
    "        actions = np.reshape(np.array([ e.action for e in rollout ]), [len(states),])\n",
    "        rewards = np.reshape(np.array([ e.reward for e in rollout ]), [len(states),])\n",
    "        next_states = np.array([ np.asarray(e.next_state) for e in rollout ])\n",
    "\n",
    "        value_s_next = self.estimate(next_states)\n",
    "        value_s_next = np.reshape(value_s_next, [len(next_states),])\n",
    "\n",
    "        # TODO: TD Target\n",
    "        td_target = rewards + self.gamma * value_s_next\n",
    "        \n",
    "        td_error, _ = self.sess.run([self.td_error_out, self.train_op_critic], feed_dict={self.state: states,\n",
    "                                                                                          self.td_target: td_target})\n",
    "        td_error = td_error.reshape(-1)\n",
    "        \n",
    "        self.sess.run(self.train_op_actor, feed_dict={self.state: states, \n",
    "                                                      self.action: actions,\n",
    "                                                      self.td_error_in: td_error})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cee2de7f2a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartpoleEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m agent = ActorCriticAgent(sess=sess, \n",
      "\u001b[0;32m/home/recharrs/env_apple/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_pruned_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_nesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/recharrs/env_apple/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewDeprecatedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/recharrs/env_apple/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    464\u001b[0m           \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "LR_A = 0.001\n",
    "LR_C = 0.01\n",
    "GAMMA = 0.99\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "env = CartpoleEnvironment()\n",
    "agent = ActorCriticAgent(sess=sess, \n",
    "                       n_states=env.observation_space.shape[0],\n",
    "                       n_actions=env.action_space.n,\n",
    "                       n_hiddens=20,\n",
    "                       lr_a=LR_A,\n",
    "                       lr_c=LR_C,\n",
    "                       gamma=GAMMA)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    return agent.act([s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_history_reward(history):\n",
    "    arr = np.asarray(history)\n",
    "    return arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 500000\n",
    "\n",
    "episode_reward = 0.0\n",
    "history_episode_rewards = deque(maxlen=100)\n",
    "episode = 0\n",
    "\n",
    "plot_history_episode_rewards = []\n",
    "\n",
    "env.reset()\n",
    "for iter in range(MAX_ITERATIONS):\n",
    "    rollout = do_step(env=env, policy=policy)\n",
    "    agent.train(rollout=rollout)\n",
    "    \n",
    "    episode_reward += rollout[0].reward\n",
    "    if rollout[0].done:\n",
    "        history_episode_rewards.append(episode_reward)\n",
    "        plot_history_episode_rewards.append(episode_reward)\n",
    "        mean_rewards = eval_history_reward(history_episode_rewards)\n",
    "        print('Episode %d: Reward = %f, Mean reward (over %d episodes) = %f' % (episode, \n",
    "                                                                                episode_reward,\n",
    "                                                                                len(history_episode_rewards),\n",
    "                                                                                mean_rewards))\n",
    "        env.reset()\n",
    "        episode += 1\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        if mean_rewards > 195.0:\n",
    "            print('Pass')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    \n",
    "    ax.set(xlabel='Episode', ylabel='Reward', title=name)\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(\"%s.png\" % name)\n",
    "    plt.show()\n",
    "    \n",
    "plot(range(episode), plot_history_episode_rewards, 'Actor-Critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "while True:\n",
    "    rollout = do_step(env=env, policy=policy)\n",
    "    agent.train(rollout=rollout)\n",
    "    env.render()\n",
    "    if rollout[0].done:\n",
    "        env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
