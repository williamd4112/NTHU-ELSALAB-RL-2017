{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3-1: REINFORCE\n",
    "    In this lab, you need to implement a REINFORCE algorithm with Tensorflow and solve OpenAI Gym CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole_env import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Define the data structure of experience\n",
    "Experience = namedtuple('Experience', 'state action reward next_state done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```discount``` function to compute discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards, gamma):\n",
    "    '''\n",
    "    param rewards: a rewards numpy array\n",
    "    param gamma: discount factor\n",
    "    '''\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    # TODO: Calculate discounted rewards\n",
    "    discounted_rewards = rewards\n",
    "    s = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        #\n",
    "        # reward = [1, 2, 3]\n",
    "        # discounted_rewards = [\n",
    "        #    1 + gamma * 2 + gamma * gamma * 3, \n",
    "        #    2 + gamma * 3,\n",
    "        #    3 ]\n",
    "        #\n",
    "        \n",
    "        s = s * gamma + rewards[i]\n",
    "        discounted_rewards[i] = s\n",
    "\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```do_rollout``` function to collect rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rollout(env, policy, render=False):\n",
    "    '''\n",
    "    Collect a rollout from env with policy\n",
    "    \n",
    "    param env: RL Environment\n",
    "    param policy: a function parameterized by environment state, return a action\n",
    "    return a list of (state, action, reward, next_state, done)\n",
    "    '''\n",
    "    # Initialize done as False\n",
    "    done = False\n",
    "    \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Empty list\n",
    "    rollout = []\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Render the environment (slow)\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        rollout.append(Experience(state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```ReinforceAgent``` following ```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(object):\n",
    "    def __init__(self, sess, n_states, n_actions, n_hiddens, lr, gamma):\n",
    "        '''\n",
    "        param sess: tf session\n",
    "        param n_states: dim of states\n",
    "        param n_actions: dim of actions space\n",
    "        param n_hiddens: dim of hidden state\n",
    "        '''\n",
    "        self.sess = sess\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "       \n",
    "        self.state = tf.placeholder(shape=[None, n_states], dtype=tf.float32)\n",
    "        self.value = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        # TODO: Declare 1-st hidden layer\n",
    "        # Define a fully-connected layer with:\n",
    "        # input = self.state\n",
    "        # n_units = self.n_hiddens\n",
    "        # activation = relu\n",
    "        # weight_initializer = random_normal(0.0, 0.1)\n",
    "        # bias_initializer = constant (0.1)\n",
    "        self.h = tf.layers.dense(\n",
    "            inputs=self.state,\n",
    "            units=n_hiddens,   \n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1), \n",
    "            bias_initializer=tf.constant_initializer(0.1), \n",
    "            name='h')\n",
    "        \n",
    "        # Declare 1-st hidden layer\n",
    "        # Define a fully-connected layer with:\n",
    "        # input = 1-st hidden layer\n",
    "        # n_units = self.n_actions\n",
    "        # activation = relu\n",
    "        # weight_initializer = random_normal(0.0, 0.1)\n",
    "        # bias_initializer = constant (0.1)\n",
    "        self.policy = tf.layers.dense(\n",
    "            inputs=self.h,\n",
    "            units=n_actions,    \n",
    "            activation=tf.nn.softmax,  \n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1), \n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='policy')\n",
    "        \n",
    "        # TODO: negative log probability\n",
    "        neg_log = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=self.action, logits=self.policy)  \n",
    "        \n",
    "        # TODO: policy gradient loss function\n",
    "        self.loss = tf.reduce_mean(neg_log * self.value)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def layer(self, inputs, n_input, n_units, activation=tf.nn.relu):\n",
    "        weights = tf.Variable(\n",
    "            tf.random_normal([n_input, n_units], mean=0.0, stddev=0.1), name=\"weights\")\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[n_units]))\n",
    "        outputs = activation(tf.matmul(inputs, weights) + bias) \n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def act(self, s):\n",
    "        '''\n",
    "        param s: a np.ndarray with shape [n_batches, n_states]\n",
    "        return a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Softmax stochastic policy\n",
    "        self.probs = tf.nn.softmax(self.policy)\n",
    "        probs = sess.run(self.probs, feed_dict={self.state: s})\n",
    "        action = np.random.choice(self.n_actions, 1, p=probs[0])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self, rollout):\n",
    "        '''\n",
    "        param rollout: a list o\n",
    "        '''\n",
    "        states = np.array([np.asarray(e.state) for e in rollout ])\n",
    "        actions = np.squeeze(np.array([ e.action for e in rollout ]))\n",
    "        rewards = np.array([ e.reward for e in rollout ])\n",
    "        discounted_rewards = discount(rewards, gamma=self.gamma)\n",
    "        \n",
    "        self.sess.run(self.train_op, feed_dict={self.state: states,\n",
    "                                                self.action: actions,\n",
    "                                                self.value: discounted_rewards})\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "GAMMA = 0.99\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "env = CartpoleEnvironment()\n",
    "agent = ReinforceAgent(sess=sess, \n",
    "                       n_states=env.observation_space.shape[0],\n",
    "                       n_actions=env.action_space.n,\n",
    "                       n_hiddens=20,\n",
    "                       lr=LR,\n",
    "                       gamma=GAMMA)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    return agent.act([s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_episode_reward(rollout):\n",
    "    rewards = [ e.reward for e in rollout ]\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_history_reward(history):\n",
    "    arr = np.asarray(history)\n",
    "    return arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReinforceAgent' object has no attribute 'probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b5f1419626c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2764a53d5bbf>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(env, policy, render)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-61b8b53525df>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d9c691602b45>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# TODO: Softmax stochastic policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#self.probs = tf.nn.softmax(self.policy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReinforceAgent' object has no attribute 'probs'"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 100000\n",
    "\n",
    "episode_reward = 0.0\n",
    "history_episode_rewards = deque(maxlen=100)\n",
    "\n",
    "plot_history_episode_rewards = []\n",
    "\n",
    "for iter in range(MAX_ITERATIONS):\n",
    "    rollout = do_rollout(env=env, policy=policy, render=False)\n",
    "    agent.train(rollout=rollout)\n",
    "    \n",
    "    episode_reward = calculate_episode_reward(rollout)\n",
    "    history_episode_rewards.append(episode_reward)\n",
    "    plot_history_episode_rewards.append(episode_reward)\n",
    "    mean_rewards = eval_history_reward(history_episode_rewards)\n",
    "    \n",
    "    print('Episode %d: Reward = %f, Mean reward (over %d episodes) = %f' % (iter, \n",
    "                                                                            episode_reward,\n",
    "                                                                            len(history_episode_rewards),\n",
    "                                                                            mean_rewards))\n",
    "    if mean_rewards > 195.0:\n",
    "        print('Pass')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    \n",
    "    ax.set(xlabel='Episode', ylabel='Reward', title=name)\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(\"%s.png\" % name)\n",
    "    plt.show()\n",
    "    \n",
    "plot(range(episode), plot_history_episode_rewards, 'REINFORCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    do_rollout(env=env, policy=policy, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
