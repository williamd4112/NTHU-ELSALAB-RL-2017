{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3-1: REINFORCE\n",
    "    In this lab, you need to implement a REINFORCE algorithm with Tensorflow and solve OpenAI Gym CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole_env import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Define the data structure of experience\n",
    "Experience = namedtuple('Experience', 'state action reward next_state done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```discount``` function to compute discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards, gamma):\n",
    "    '''\n",
    "    param rewards: a rewards numpy array\n",
    "    param gamma: discount factor\n",
    "    '''\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    \n",
    "    # TODO: Calculate discounted rewards\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```do_rollout``` function to collect rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rollout(env, policy, render=False):\n",
    "    '''\n",
    "    Collect a rollout from env with policy\n",
    "    \n",
    "    param env: RL Environment\n",
    "    param policy: a function parameterized by environment state, return a action\n",
    "    return a list of (state, action, reward, next_state, done)\n",
    "    '''\n",
    "    # Initialize done as False\n",
    "    done = False\n",
    "    \n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Empty list\n",
    "    rollout = []\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Render the environment (slow)\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        rollout.append(Experience(state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement ```ReinforceAgent``` following ```TODO```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(object):\n",
    "    def __init__(self, sess, n_states, n_actions, n_hiddens, lr, gamma):\n",
    "        '''\n",
    "        param sess: tf session\n",
    "        param n_states: dim of states\n",
    "        param n_actions: dim of actions space\n",
    "        param n_hiddens: dim of hidden state\n",
    "        '''\n",
    "        self.sess = sess\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "       \n",
    "        self.state = tf.placeholder(shape=[None, n_states], dtype=tf.float32)\n",
    "        self.value = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        # TODO: Declare 1-st hidden layer\n",
    "        # Define a fully-connected layer with:\n",
    "        # input = self.state\n",
    "        # n_units = self.n_hiddens\n",
    "        # activation = relu\n",
    "        # weight_initializer = random_normal(0.0, 0.1)\n",
    "        # bias_initializer = constant (0.1)\n",
    "        \n",
    "        \n",
    "        # TODO: Declare 1-st hidden layer\n",
    "        # Define a fully-connected layer with:\n",
    "        # input = 1-st hidden layer\n",
    "        # n_units = self.n_actions\n",
    "        # activation = relu\n",
    "        # weight_initializer = random_normal(0.0, 0.1)\n",
    "        # bias_initializer = constant (0.1)\n",
    "        \n",
    "        # TODO: negative log probability \n",
    "        \n",
    "         \n",
    "        # TODO: policy gradient loss function\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        self.train_op = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def act(self, s):\n",
    "        '''\n",
    "        param s: a np.ndarray with shape [n_batches, n_states]\n",
    "        return a batch of actions with shape [n_batches,]\n",
    "        '''\n",
    "        # TODO: Softmax stochastic policy\n",
    "    \n",
    "    def train(self, rollout):\n",
    "        '''\n",
    "        param rollout: a list o\n",
    "        '''\n",
    "        states = np.array([ np.asarray(e.state) for e in rollout ])\n",
    "        actions = np.squeeze(np.array([ e.action for e in rollout ]))\n",
    "        rewards = np.array([ e.reward for e in rollout ])\n",
    "        discounted_rewards = discount(rewards, gamma=self.gamma)\n",
    "        \n",
    "        self.sess.run(self.train_op, feed_dict={self.state: states,\n",
    "                                                self.action: actions,\n",
    "                                                self.value: discounted_rewards})\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-16 18:56:28,425] Making new env: CartPole-v0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReinforceAgent' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cffd789f4a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                        \u001b[0mn_hiddens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        gamma=GAMMA)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1c678f5fda43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, n_states, n_actions, n_hiddens, lr, gamma)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReinforceAgent' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "GAMMA = 0.99\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "env = CartpoleEnvironment()\n",
    "agent = ReinforceAgent(sess=sess, \n",
    "                       n_states=env.observation_space.shape[0],\n",
    "                       n_actions=env.action_space.n,\n",
    "                       n_hiddens=20,\n",
    "                       lr=LR,\n",
    "                       gamma=GAMMA)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(s):\n",
    "    return agent.act([s])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_episode_reward(rollout):\n",
    "    rewards = [ e.reward for e in rollout ]\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_history_reward(history):\n",
    "    arr = np.asarray(history)\n",
    "    return arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 100000\n",
    "\n",
    "episode_reward = 0.0\n",
    "history_episode_rewards = deque(maxlen=100)\n",
    "\n",
    "plot_history_episode_rewards = []\n",
    "\n",
    "for iter in range(MAX_ITERATIONS):\n",
    "    rollout = do_rollout(env=env, policy=policy, render=False)\n",
    "    agent.train(rollout=rollout)\n",
    "    \n",
    "    episode_reward = calculate_episode_reward(rollout)\n",
    "    history_episode_rewards.append(episode_reward)\n",
    "    plot_history_episode_rewards.append(episode_reward)\n",
    "    mean_rewards = eval_history_reward(history_episode_rewards)\n",
    "    \n",
    "    print('Episode %d: Reward = %f, Mean reward (over %d episodes) = %f' % (iter, \n",
    "                                                                            episode_reward,\n",
    "                                                                            len(history_episode_rewards),\n",
    "                                                                            mean_rewards))\n",
    "    if mean_rewards > 195.0:\n",
    "        print('Pass')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y)\n",
    "    \n",
    "    ax.set(xlabel='Episode', ylabel='Reward', title=name)\n",
    "    ax.grid()\n",
    "\n",
    "    fig.savefig(\"%s.png\" % name)\n",
    "    plt.show()\n",
    "    \n",
    "plot(range(episode), plot_history_episode_rewards, 'REINFORCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    do_rollout(env=env, policy=policy, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
